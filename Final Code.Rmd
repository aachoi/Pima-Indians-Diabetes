---
title: "Stats101C Project"
author: "Annie Choi and Justin Kim"
date: "12/13/2018"
output: pdf_document
---

# Initial Attempt
Logistic Model Using Uncleaned Data
```{r}
library(leaps)
library(MASS)
library(mixlm)
library(plyr)
pdata <- read.csv(file = "HTrainLast.csv", stringsAsFactors = TRUE)
# Get rid of the observation column 
pdata = pdata[, -1]

# Create our training (90%) and validation (10%) set
set.seed(1234) # for reproducibility
traini = sample(1:3500,3500*0.9,replace=F)
ptrain = pdata[traini,]
ptest = pdata[-traini,]

# ??? apparently only predictors that we think are important
# We need to explore what the full logistic regression is like so we know which predictors are important
FullMod = glm(as.factor(affordabilitty)~(MSSubClass)+LotFrontage+LotArea+LandContour+LandSlope+BldgType+OverallQual+OverallCond+YearBuilt+YearRemodAdd+MasVnrArea+Foundation+TotalBsmtSF+X1stFlrSF+X2ndFlrSF+BsmtFullBath+BsmtHalfBath+FullBath+HalfBath+BedroomAbvGr+KitchenAbvGr+TotRmsAbvGrd+Functional+Fireplaces+GarageArea+OpenPorchSF+EnclosedPorch,family=binomial(),data=ptrain)

summary(FullMod)

predictprobs = predict(FullMod,ptest, type='response')
predictlogit = rep('1',length(predictprobs))
predictlogit[predictprobs>0.5] = '0'
confmatrix <- table(predictlogit,as.factor(ptest$affordabilitty))
confmatrix
sum(diag(confmatrix)/ sum(confmatrix)) # Accuracy
rm(list = ls())
```

# Data Cleaning
```{r}
library(reshape2)
library(ggplot2)
library(purrr)
library(tidyr)
library(corrplot)
library(plyr)
library(forcats) 
library(randomForest)
library(readr)
library(dplyr)
library(ggpubr)

# Import training and testing data
real_training <- read.csv(file = "HTrainLast.csv", stringsAsFactors = TRUE)
real_testing <- read.csv(file = "HTestLastNoY.csv", stringsAsFactors = TRUE)
real_testing$affordabilitty = rep(NA,nrow(real_testing))

# Combine training & testing
combined = rbind(real_training,real_testing)

# Get rid of the observation column 
combined = combined[, -1]

summary(combined)

# Manually changing predictors to factors based on Data Description
combined$MSSubClass <- as.factor(combined$MSSubClass)


# Get rid of utilities, MiscFeature, MiscVal
which(names(combined ) =='Utilities' | names(combined ) =='MiscFeature' |  names(combined ) =='MiscVal')
combined=combined[,-c(9,74,75)] 

###### Dealing with Missing Data

# Changing all the NA's into "None" (another factor level)
#  Alley - 4662 NA's (no alley access)
combined$Alley <- fct_explicit_na(combined$Alley, na_level = "None")
# BsmtQual - 140 NA's (no basement)
combined$BsmtQual <- fct_explicit_na(combined$BsmtQual, na_level = "None")
# BsmtCond - 137 NA's (no basement)
combined$BsmtCond <- fct_explicit_na(combined$BsmtCond, na_level = "None")
# Bsmt Exposure - 137 NA's (no basement)
combined$BsmtExposure <- fct_explicit_na(combined$BsmtExposure, na_level = "None")
# Bsmt Fintype1 - 134 NA's (no basement)
combined$BsmtFinType1 <- fct_explicit_na(combined$BsmtFinType1, na_level = "None")
# Bsmt Fintype2 - 135 NA's (no basement)
combined$BsmtFinType2 <- fct_explicit_na(combined$BsmtFinType2, na_level = "None")
# FireplaceQu - 2455 NA's (no fireplace)
combined$FireplaceQu <- fct_explicit_na(combined$FireplaceQu, na_level = "None")
# Fence - 3988 NA's (no fence)
combined$Fence <- fct_explicit_na(combined$Fence, na_level = "None")

# CATEGORICAL
# If NA's in GarageType, GarageFinish, GarageQual, GarageCond all mean no garage, they should have the same number of missing values, we investigate further
combined[(!is.na(combined$GarageType) & is.na(combined$GarageFinish)),c('GarageType','GarageFinish', 'GarageCars', 'GarageArea','GarageCond','GarageQual')]
# We see that there are two observations, that were mislabeled as having detached garages, when in fact GarageFinish, GarageCars, GarageArea, GarageCond, GarageQual indicate that a garage doesn't exist
# GarageType - 252 NA's (no garage)
combined$GarageType[c(833, 4009)] <- NA # change the 2 outliers in GarageType to NA
combined$GarageType <- fct_explicit_na(combined$GarageType, na_level = "None")
# GarageFinish - 254 NA's (no garage)
combined$GarageFinish <- fct_explicit_na(combined$GarageFinish, na_level = "None")
# GarageQual - 254 NA's (no garage)                                      
combined$GarageQual <- fct_explicit_na(combined$GarageQual, na_level = "None")
# GarageCond - 254 NA's (no garage)
combined$GarageCond <- fct_explicit_na(combined$GarageCond, na_level = "None")
# Garage Year Built - 254 NA's
# NA for GarageYrBlt shows "none" for GarageType, GarageFinish, GarageCars, GarageArea, GarageQual, GarageCond -> means no garage
combined[which(is.na(combined$GarageYrBlt)),c('GarageYrBlt','GarageType', 'GarageFinish', 'GarageCars', 'GarageArea','GarageQual','GarageCond')]
combined$GarageYrBlt[is.na(combined$GarageYrBlt)] = 2018
# OUTLIER: since we saw that for GarageYrBlt max = 2207
combined[which(combined$GarageYrBlt > 2018),c('GarageYrBlt','YearBuilt','GarageType', 'GarageFinish', 'GarageCars', 'GarageArea','GarageQual','GarageCond')]
# Can see these outlier visually
garageyrblt_outlier <- ggplot(data = combined, aes(x = GarageQual, y = GarageYrBlt)) +  geom_boxplot(aes(fill = GarageQual), show.legend = FALSE)
# We know that there's one outlier, and it has a garage, but the GarageYrBlt is wrong, so we just assign GarageYrBlt = YearBuilt
combined$GarageYrBlt[combined$GarageYrBlt==2207] = combined$YearBuilt[combined$GarageYrBlt==2207]
# Visually see impact of removing outlier 
garageyrblt_outlier_after <- ggplot(data = combined, aes(x = GarageQual, y = GarageYrBlt)) +  geom_boxplot(aes(fill = GarageQual), show.legend = FALSE)
# Creating figure 2
figure <- ggarrange(garageyrblt_outlier, garageyrblt_outlier_after, 
          labels = c("A", "B"),
          ncol = 2, nrow = 1)

garageyrblt<- annotate_figure(figure,
                top = text_grob("Comparison of Garage Year Built Across Garage Quality", face = "bold", size = 12, hjust = 0.5),
                bottom = text_grob("Figure 2A: Identifying the outlier in GarageYrBlt. The outlier observation has a GarageQual ‘TA’, meaning  \n that it has a garage on the property. We set the GarageYrBlt value equal to the YearBuilt value. \n \n Figure 2B: Reassigning our outlier positively impacted our predictor GarageYrBlt.",hjust = 0, x = 0, face = "italic", size = 10),
                fig.lab = "Figure 2", fig.lab.face = "bold"
                )
garageyrblt
ggexport(plotlist = list(garageyrblt),filename = "garageyrblt.png")
rm(list = c("garageyrblt_outlier", "garageyrblt_outlier_after", "figure", "garageyrblt"))

# PoolQC = 3490 NA's (no pool)
# Although all PoolQC with NA's are supposed to mean that they don't have a pool, we found 6 cases, where PoolQC = NA, even though they have a pool area
combined[(combined$PoolArea > 0) & is.na(combined$PoolQC),c('PoolQC','PoolArea')]
# Can see these outliers visually 
poolqc_outlier <- ggplot(data = combined, aes(x = PoolQC, y = PoolArea)) +  geom_boxplot(aes(fill = PoolQC), show.legend = FALSE) +  scale_fill_brewer(palette="Dark2")
# Calculate the mean pool size for each level of pool condition
combined[,c('PoolQC','PoolArea')] %>% group_by(PoolQC) %>%
  summarise(mean = mean(PoolArea), counts = n()) 
# Assign these pools a quality based on the mean of that size of a pool 
combined[644,'PoolQC'] = 'Fa' #561
combined[1052,'PoolQC'] = 'Ex' #368
combined[1192,'PoolQC'] = 'Fa' #561
combined[1425,'PoolQC'] = 'Ex' #444
combined[4555,'PoolQC'] = 'Fa' #561
combined[4668,'PoolQC'] = 'Fa' #561
# Assign the rest of the PoolQC that have a PoolArea=0, to none
combined$PoolQC <- fct_explicit_na(combined$PoolQC, na_level = "None")
# Visually see impact of removing outlier 
poolqc_outlier_after <- ggplot(data = combined, aes(x = PoolQC, y = PoolArea)) +  geom_boxplot(aes(fill = PoolQC), show.legend = FALSE) +  scale_fill_brewer(palette="Dark2")
# Creating Figure 3
figure <- ggarrange(poolqc_outlier, poolqc_outlier_after, 
          labels = c("A", "B"),
          ncol = 2, nrow = 1)
poolqc<- annotate_figure(figure,
                top = text_grob("Comparison of Pool Area Across Pool Quality", face = "bold", size = 12, hjust = 0.5),
                bottom = text_grob("Figure 3A: Identifying the outliers in PoolQC. The 6 outlier observations have no pool, but 3 unique Pool \n Area values. \n \n Figure 3B: Relabeling our outliers positively impacted our predictor PoolQC.",hjust = 0, x = 0, face = "italic", size = 10),
                fig.lab = "Figure 3", fig.lab.face = "bold",
                )
poolqc
ggexport(plotlist = list(poolqc),filename = "poolqc.png")
rm(list = c("poolqc_outlier", "poolqc_outlier_after", "figure", "poolqc"))

# 1st approach to MasVnrType and MasVnrArea
# Replacing MasVnrType NA's
#MasVnrdata = cbind(MasVnrType=training$MasVnrType,NoNaData)
#MasVnrtrain = MasVnrdata[complete.cases(MasVnrdata),]
#MasVnrtest = MasVnrdata[is.na(MasVnrdata),]
#n = nrow(MasVnrtrain)
#traini = sample(1:n,3466,replace=FALSE)
#Mastn = MasVnrtrain[traini,]
#Mastt = MasVnrtrain[-traini,]
#rfMasVnr = randomForest(MasVnrType~.,data=MasVnrtrain,mtry=54,importance=TRUE) #Creating Model to replace NA
#pred.Vnr = predict(rfMasVnr,newdata=MasVnrtest)
#training$MasVnrType[is.na(training$MasVnrType)] = pred.Vnr #Changing NA to predicted 

# Replacing MasVnrArea 
#library(gbm)

#Masdata = cbind(MasVnrArea=training$MasVnrArea,NoNaData)
#Mastrain = Masdata[complete.cases(Masdata),]
#Mastest = Masdata[is.na(Masdata),]
#n = nrow(Mastrain)
#traini = sample(1:n,3468,replace=FALSE)
#Mastn = Mastrain[traini,]
#Mastt = Mastrain[-traini,]
#Masgbm = gbm(MasVnrArea~.,data=Mastrain,distribution="gaussian",n.tree=5000,interaction.depth=4)
#pred.Mas = predict(Masgbm,newdata=Mastest,n.tree=5000)
#training$MasVnrArea[is.na(training$MasVnrArea)] = pred.Mas

# 2nd Attempt for MasVnrArea and MasVnrType
# Replacing NA for MasVnrArea and MasVnrType
# MasVnrType - 49 NA's (no masonry)   
combined$MasVnrType <- fct_explicit_na(combined$MasVnrType, na_level = "None")
# MasVnrArea - 46 NA's (no masonry area)  
combined$MasVnrArea[is.na(combined$MasVnrArea)] = 0
# There's many instances, where MasVnrType is None, but it has a MasVnrArea, so we set these MasVnrType = "Other"
# Visual representation of outliers
masvnrtype_outlier <- ggplot(data = combined, aes(x = MasVnrType, y = MasVnrArea)) +  geom_boxplot(aes(fill = MasVnrType), show.legend = FALSE) +  scale_fill_brewer(palette="YlOrRd")
# Outputting the 16 outlier cases
combined[(combined$MasVnrArea > 0) &(combined$MasVnrType =="None"), c("MasVnrType","MasVnrArea")]
# The ones where MasVnrArea = 1, must be an error, so we set it equal to 0.
combined$MasVnrArea[(combined$MasVnrArea == 1)] = 0
# Add a level to MasVnrType: "Other"
levels(combined$MasVnrType) = c(levels(combined$MasVnrType),"Other")
combined$MasVnrType[combined$MasVnrType=="None" & combined$MasVnrArea!=0] = "Other"
# Visually see impact of removing outlier 
masvnrtype_outlier_after <- ggplot(data = combined, aes(x = MasVnrType, y = MasVnrArea)) +  geom_boxplot(aes(fill = MasVnrType), show.legend = FALSE) + scale_fill_brewer(palette="YlOrRd")
# Creating Figure 4
figure <- ggarrange(masvnrtype_outlier, masvnrtype_outlier_after, 
          labels = c("A", "B"),
          ncol = 2, nrow = 1)
masvnrtype <- annotate_figure(figure,
                top = text_grob("Comparison of Masonry Veneer Area Across Type", face = "bold", size = 12, hjust = 0.5),
                bottom = text_grob("Figure 4A: Identifying the outliers in MasVnrType. The 16 outlier observations are labeled in the ‘None’ \n level of MasVnrType. This indicates, they have no Masonry Veneer, but they have a corresponding \n Masonry Veneer Area. We reclassify them as “Other”. \n \n Figure 4B: Relabeling our outliers positively impacted our predictor MasVnrType. The classification of a \n lack of Masonry Veneer is consistent between MasVnrArea and MasVnrType.",hjust = 0, x = 0, face = "italic", size = 10),
                fig.lab = "Figure 4", fig.lab.face = "bold",
                )
masvnrtype
ggexport(plotlist = list(masvnrtype),filename = "masvnrtype.png")
rm(list = c("masvnrtype_outlier", "masvnrtype_outlier_after", "figure", "masvnrtype"))

# NUMERICAL
# For Basement, we see that the same 2 observations are missing, and see that their BmtQual = None, which means that they have no basement, so we set the following equal to 0
combined[is.na(combined$BsmtFinSF1), 29:37]
# BsmtFinSF1 - 2 NA's
combined$BsmtFinSF1[is.na(combined$BsmtFinSF1)] = 0
# BsmtFinSF2 - 2 NA's
combined$BsmtFinSF2[is.na(combined$BsmtFinSF2)] = 0
# BsmtUnfSF - 2 NA's
combined$BsmtUnfSF[is.na(combined$BsmtUnfSF)] = 0
# TotalBsmtSF - 2 NA's
combined$TotalBsmtSF[is.na(combined$TotalBsmtSF)] = 0
# For Basement Bathrooms, we see that the same 4 observations are missing, and see that their BmtQual = None, which means that they have no basement, so we set them to 0
which(is.na(combined$BsmtFullBath)) == which(is.na(combined$BsmtHalfBath))
combined[is.na(combined$BsmtHalfBath),c('BsmtHalfBath','BsmtFullBath','BsmtQual')] 
# Basement Full Bath - 4 NA's
combined$BsmtFullBath[is.na(combined$BsmtFullBath)] = 0
# Basement Half Bath - 4 NA's
combined$BsmtHalfBath[is.na(combined$BsmtHalfBath)] = 0
# NA for GarageCars & GarageArea shows "none" for GarageType, GarageFinish, GarageCars, GarageArea, GarageQual, GarageCond -> means no garage
combined[(is.na(combined$GarageCars) & is.na(combined$GarageArea)),c('GarageType','GarageFinish', 'GarageCars', 'GarageArea','GarageCond','GarageQual')]
# Garage Cars - 2 NA's
combined$GarageCars[is.na(combined$GarageCars)] = 0
# Garage Area - 2 NA's
combined$GarageArea[is.na(combined$GarageArea)] = 0

# Values that can't be relabeled as "None"/ 0 , we have to sample
# First attempt: sample everything
# MSZoning - 5 NA's
combined$MSZoning[is.na(combined$MSZoning)] = sample(levels(combined$MSZoning),sum(is.na(combined$MSZoning),na.rm=TRUE),replace=TRUE)
# LotFrontage - 815 NA's 
combined$LotFrontage[is.na(combined$LotFrontage)] = median(combined$LotFrontage,na.rm=TRUE)
# Electrical - 2 NA's
#combined$Electrical[is.na(combined$Electrical)] = sample(levels(combined$Electrical),sum(is.na(combined$Electrical),na.rm=TRUE),replace=TRUE)
# KitchenQual - 2 NA's
#combined$KitchenQual[is.na(combined$KitchenQual)] = sample(levels(combined$KitchenQual),sum(is.na(combined$KitchenQual),na.rm=TRUE),replace=TRUE)
# Functional - 3 NA's
#combined$Functional[is.na(combined$Functional)] = sample(levels(combined$Functional),sum(is.na(combined$Functional),na.rm=TRUE),replace=TRUE)
# Exterior 1st - 1 NA
#combined$Exterior1st[is.na(combined$Exterior1st)] = sample(levels(combined$Exterior1st),sum(is.na(commbined$Exterior1st),na.rm=TRUE),replace=TRUE)
# Exterior 2nd - 1 NA
#combined$Exterior2nd[is.na(combined$Exterior2nd)] = sample(levels(combined$Exterior2nd),sum(is.na(combined$Exterior2nd),na.rm=TRUE),replace=TRUE)
# Sale Type - 1 NA
#combined$SaleType[is.na(combined$Exterior2nd)] = sample(levels(combined$Exterior2nd),sum(is.na(combined$Exterior2nd),na.rm=TRUE),replace=TRUE)

# Second attempt:
# Missing values < 5, reassign
# Missing values >= 5, randomForest/boosting

# Values with < 5 missing values, but the distribution of categories was heavily skewed, we filled it in with the mode
# Electrical - 2 NA's
  # Visual
electrical <- ggplot(data = combined, aes(Electrical)) +  geom_bar(aes(fill = Electrical), show.legend = FALSE)  + scale_fill_brewer(palette="Blues")
combined$Electrical[is.na(combined$Electrical)] = "SBrkr"

# KitchenQual - 2 NA's
  # Visual
kitchenqual <- ggplot(data = combined, aes(KitchenQual)) +  geom_bar(aes(fill = KitchenQual), show.legend = FALSE) + scale_fill_brewer(palette="BrBG")
combined$KitchenQual[is.na(combined$KitchenQual)] = "TA"

# Functional - 3 NA's
  # Visual
functional <- ggplot(data = combined, aes(Functional)) +  geom_bar(aes(fill = Functional), show.legend = FALSE) + scale_fill_brewer(palette="PiYG")
combined$Functional[is.na(combined$Functional)] = "Typ"

# Exterior 1st - 1 NA
  # Visual
exterior1st <- ggplot(data = combined, aes(Exterior1st)) +  geom_bar(aes(fill = Exterior1st), show.legend = FALSE) + scale_fill_brewer(palette="BrBG") + theme(axis.text.x = element_text(angle = 45, hjust = 1))
combined$Exterior1st[is.na(combined$Exterior1st)] = "VinylSd"

# Exterior 2nd - 1 NA
  # Visual
exterior2nd <- ggplot(data = combined, aes(Exterior2nd)) +  geom_bar(aes(fill = Exterior2nd), show.legend = FALSE)  + scale_fill_brewer(palette="YlGn") + theme(axis.text.x = element_text(angle = 45, hjust = 1))
combined$Exterior2nd[is.na(combined$Exterior2nd)] = "VinylSd"
# Sale Type - 1 NA
 # Visual
saletype<- ggplot(data = combined, aes(SaleType)) +  geom_bar(aes(fill = SaleType), show.legend = FALSE)  +  scale_fill_brewer(palette="Spectral") + theme(axis.text.x = element_text(angle = 45, hjust = 1))
# Since SaleType is correlated w/ SaleCondition, we see Normal is most associated with "WD"
combined[is.na(combined$SaleType),c('SaleType', 'SaleCondition')]
table(combined$SaleCondition, combined$SaleType) # table showing Normal & "WD"
ggplot(combined, aes(SaleCondition)) + geom_bar(aes(fill = SaleType)) # graph showing Normal & "WD"
# Replace all NA sale type w/ "WD"
combined$SaleType[is.na(combined$SaleType)] = 'WD'


# Figure 5
figure <- ggarrange(electrical, kitchenqual, functional, exterior1st, exterior2nd, saletype,
          labels = c("A", "B", "C", "D", "E","F"),
          ncol = 2, nrow = 3)
fig5 <- annotate_figure(figure,
                top = text_grob("Exploratory Analysis for Imputation", face = "bold", size = 12, hjust = 0.5),
                bottom = text_grob(" All figures: There is a clear factor level where the majority of the observations lie. We assign the missing \n values for each predictor to the level with the highest frequency.",hjust = 0, x = 0, face = "italic", size = 10),
                fig.lab = "Figure 5", fig.lab.face = "bold",
                )
ggexport(plotlist = list(fig5),filename = "figure5.png")
rm(list = c("electrical", "kitchenqual", "functional", "exterior1st", "exterior2nd","saletype", "figure", "fig5"))

##################################################
# Missing values >= 5
# Prediction Model
# Getting the data without NA's
NoNaData = combined[,colSums(is.na(combined))==0]
NoNaNumeric = select_if(NoNaData, is.numeric) # numeric
NoNaCat = select_if(NoNaData, is.factor) # categorical

# Replacing MSZoning NA's
MSdata = cbind(MSZoning=combined$MSZoning,NoNaData)
MStrain = MSdata[complete.cases(MSdata),]
MStest = MSdata[is.na(MSdata),]
rfMS = randomForest(MSZoning~.,data=MStrain,mtry=54,importance=TRUE)
pred.MS = predict(rfMS,newdata=MStest)
training$MSZoning[is.na(training$MSZoning)] = pred.MS

# Replacing LotFrontage NA
library(gbm)
Lotdata = cbind(LotFrontage=training$LotFrontage,NoNaData)
Lottrain = Lotdata[complete.cases(Lotdata),]
Lottest = Lotdata[is.na(Lotdata),]
n = nrow(Lottrain)
traini = sample(1:n,2930,replace=FALSE)
Lottn = Lottrain[traini,]
Lottt = Lottrain[-traini,]
Lotgbm = gbm(LotFrontage~.,data=Lottrain,distribution="gaussian",n.tree=500,interaction.depth=4)
pred.Lot = predict(Lotgbm,newdata=Lottest,n.tree=500)
training$LotFrontage[is.na(training$LotFrontage)] = pred.Lot
##################################################

###### Feature Engineering
# House Age And Yes/No for remodeled
# Remodeled (Categorical)
combined$Remodeled <- rep("Yes", nrow(combined))
combined$Remodeled[which (combined$YearRemodAdd - combined$YearBuilt==0)]<- "No"
combined$Remodeled <- as.factor(combined$Remodeled)
# House Age
# age = 2018 - yearbuilt (if not remodeled)
combined$HouseAge[which(combined$Remodeled == "No")] <- 2018 - combined$YearBuilt[which(combined$Remodeled == "No")]
# age = 2018 - yearRemodeled (if remodeled)
combined$HouseAge[which(combined$Remodeled == "Yes")] <- 2018 - combined$YearRemodAdd[which(combined$Remodeled == "Yes")]
# We can drop YearBuilt, YearRemodAdd 
combined = combined[,colnames(combined) != "YearBuilt"]
combined = combined[,colnames(combined) != "YearRemodAdd"]

# Taking out BsmtFinSF1 and 2
combined$BsmtTotalFin = combined$BsmtFinSF1 + combined$BsmtFinSF2
# We can drop BsmtFinSF1, BsmtFinSF2
combined = combined[,colnames(combined) != "BsmtFinSF2"]
combined = combined[,colnames(combined) != "BsmtFinSF1"]

# Condense levels of LotShape
combined$LotShape <- as.character(combined$LotShape)
combined$LotShape[which (combined$LotShape!= "Reg")] <- "IR"
combined$LotShape <- as.factor(combined$LotShape)

# Reorder our columns so that affordabilitty is last predictor
combined2 <- combined
combined <- cbind(combined2[,-73], affordabilitty = combined2$affordabilitty)
rm(combined2)


```

Correlation between numeric predictors
```{r}
# Splitting combined by numeric and factors
combined.numeric <- select_if(combined, is.numeric)
combined.factors =select_if(combined, is.factor) 

# Correlation between our numeric predictors
library(corrplot)
library(reshape2)
# Get upper triangle of the correlation matrix
  get_upper_tri <- function(cormat){
    cormat[lower.tri(cormat)]<- NA
    return(cormat)
  }
corr <- round(cor(combined.numeric),3)
# Visualize correlation matrix
upper_tri <- get_upper_tri(corr)
melted_corr <- melt(upper_tri, na.rm = TRUE)
corr_plot <- ggplot(data = melted_corr, aes(Var2, Var1, fill = value))+
 geom_tile(color = "white")+
 scale_fill_gradient2(low = "blue", high = "red", mid = "white", 
   midpoint = 0, limit = c(-1,1), space = "Lab", 
   name="Pearson\nCorrelation") +
  theme_minimal()+ 
 theme(axis.text.x = element_text(angle = 60, vjust = 1, 
    size = 8, hjust = 1))+ coord_fixed() + labs(title = "Correlation Matrix of Numeric Predictors") + theme(axis.title.x = element_blank()) + theme(axis.title.y = element_blank()) + theme(plot.title = element_text(hjust = 0.5))
corr_plot
ggexport(plotlist = list(corr_plot),filename = "corr_plot.png")

# See which predictors have a correlation equal to or higher than 0.7
corr[which(abs(corr) >=0.7 & abs(corr) !=1 )]
which(abs(corr) >=0.7 & abs(corr) !=1 )
temp_ind <- which(abs(corr) >=0.7 & abs(corr) !=1  , arr.ind = TRUE)
corr_table <- data.frame(predictors = paste(rownames(corr)[temp_ind[,"row"]],  colnames(corr)[temp_ind[,"col"]], sep=', '), values = corr[which(abs(corr) >=0.7 & abs(corr) !=1 )])
corr_table

# Highly correlated pairs:
# We drop GarageCars, GarageArea
# We drop X1stFlrSF, TotalBsmtSF
# We drop GrLivArea, TotRmsAbvGrd

training = combined[1:3500,]
testing = combined[3501:5000,]
training = training[complete.cases(training),]

# Visualizing different densities w/ training data
# GarageCars, GarageArea
ggplot(data = training, aes(x = GarageArea, color = affordabilitty)) + geom_density() + labs(title = "Density Plot of GarageArea by Affordabilitty") + theme(plot.title = element_text(hjust = 0.5))
ggplot(data = training, aes(x = GarageCars, color = affordabilitty)) + geom_density()  + labs(title = "Density Plot of GarageCars by Affordabilitty") + theme(plot.title = element_text(hjust = 0.5))
# X1stFlrSF, TotalBsmtSF
ggplot(data = training, aes(x = X1stFlrSF, color = affordabilitty)) + geom_density()  + labs(title = "Density Plot of 1st Floor SF by Affordabilitty") + theme(plot.title = element_text(hjust = 0.5))
ggplot(data = training, aes(x = TotalBsmtSF, color = affordabilitty)) + geom_density()  + labs(title = "Density Plot of Total Basement SF by Affordabilitty") + theme(plot.title = element_text(hjust = 0.5))
# GrLivArea, TotRmsAbvGrd 
ggplot(data = training, aes(x = GrLivArea, color = affordabilitty)) + geom_density()  + labs(title = "Density Plot of Above Ground Living SF by Affordabilitty") + theme(plot.title = element_text(hjust = 0.5))
ggplot(data = training, aes(x = TotRmsAbvGrd, color = affordabilitty)) + geom_density() + labs(title = "Density Plot of Total Rooms Above Ground by Affordabilitty") + theme(plot.title = element_text(hjust = 0.5))

# Drop GarageCars, X1stFlrSF, and TotRmsAbvGrd from our combined df
combined.numeric = combined.numeric[,colnames(combined.numeric) != "GarageCars"]
combined.numeric = combined.numeric[,colnames(combined.numeric) != "X1stFlrSF"]
combined.numeric = combined.numeric[,colnames(combined.numeric) != "TotRmsAbvGrd"]
```

PCA on the numeric variables
```{r}
library(factoextra)
library(data.table)
library(formattable)

pca = princomp(combined.numeric,cor=T)
eig.val <- get_eigenvalue(pca)
eig.val
scree.plot <- fviz_eig(pca, addlabels = TRUE, ylim = c(0,30)) # to control how many dimensions to show ncp = 25
scree.plot
# Print scree plot to a png file
ggexport(plotlist = list(scree.plot),filename = "screeplot.png")

# Make our eig value table pretty
customBlack = "#000"
customGreen = "#71CA97"
customRed = "#ff7f7f"
names(eig.val) <- c("Eigenvalue", "Variance %", "Cumulative Variance %")
eig.val <- round(eig.val, 3)
rownames(eig.val) <- c(1:29)
eig.val$Dimension <- seq(1:29)
eig.val <- eig.val[,c(4,1,2,3)]

eigenvalue_formatter <- formatter("span", style = x ~ style( 
              color = ifelse(x > 1, customGreen, ifelse(x < 1, "black", "black"))))

variance_formatter <- formatter("span", style = x ~ style( 
              color = ifelse(x > 95, customGreen, ifelse(x < 95, customRed, "black"))))

eig.table <- formattable(eig.val, align = c("l", "c","c","r"),
            list(`Dimension` = formatter("span", style = ~ style(color = "grey",font.weight = "bold")), 
                 `Eigenvalue` = eigenvalue_formatter,
                 `Cumulative Variance %` = variance_formatter))
eig.table
ggexport(plotlist = list(eig.table),filename = "eigtable.png")

x = summary(pca)
pc.comp <- pca$scores
pcdf = pc.comp[,1:23]
pcdf = data.frame(pcdf)
colnames(pcdf) = c(1:23)

# Our final dataframe after PCA
combined_pca = cbind(combined.factors[,-44],pcdf, affordabilitty = combined.factors$affordabilitty)

# Why did we drop exterior2nd
# combined_pca = training.factors[,colnames(training.factors) != "Exterior2nd"]

# Split our PCA combined data into the original testing and training set
testing1 = combined_pca[3501:5000,]
training1 = combined_pca[1:3500,]
training1 = training1[complete.cases(training1),]
```



Splitting the training to 90% and 10%
```{r}
set.seed(1234)
traini = sample(nrow(training1),nrow(training1)*0.9,replace=FALSE)
train_train = training1[traini,]
train_test = training1[-traini,]
```

Logistic Model With all predictors included using Cleaned Data
```{r}
log.mod.full <- glm(affordabilitty~.,family=binomial(),data=train_train)
summary(log.mod.full)

predictprobs = predict(log.mod.full,train_test, type='response')
predictlogit = rep('Affordable',length(predictprobs))
predictlogit[predictprobs>0.5] = 'Unaffordable'
table(predictlogit,train_test$affordabilitty)
mean(predictlogit==train_test$affordabilitty) # 94% Accuracy

# We want to keep a record of our final predictions
predictprobs = predict(log.mod.full,testing1, type='response')
predictlogit = rep('Affordable',length(predictprobs))
predictlogit[predictprobs>0.5] = 'Unaffordable'
final.prediction <- data.frame('Logistic Model Full' = predictlogit)
```




















```{r}
# How do we deal with Basement?
#combined[(is.na(combined$BsmtQual)) & !is.na(combined$BsmtCond),c('BsmtQual','BsmtCond')]


ggplot(combined, aes(Electrical)) + geom_bar()
#exploratory analysis


head(combined[,c('Exterior1st', 'Exterior2nd')])
levels(combined$Exterior1st)
levels(combined$Exterior2nd)
levels(combined$Exterior2nd)[6] <- "CemntBd"
levels(combined$Exterior1st) = c(levels(combined$Exterior1st),"Other")
levels(combined$Exterior1st)
combined[which(as.character(combined$Exterior1st) != as.character(combined$Exterior2nd)), c('Exterior1st', 'Exterior2nd')]

```





QDA and LDA #Does not work for some reason
```{r}
#plda = lda(affordabilitty~.,data=btrain1)

#predictprobs = predict(plda,ptest1)
#table(predictprobs$class,ptest1$affordabilitty)

# WITH TEST SET

#plda = lda(affordabilitty~.,data=btrain)

#predictprobs = predict(plda,ptdata)
#predictprobs$class

#Attempt3Data = 
#revalue(predictprobs$class,c("1"="Affordable","0"="Unaffordable"))
```





Logistic With Backward And Forward Selection
```{r}
library(leaps)
library(MASS)
library(mixlm)
library(plyr)

# Using variables that came out important from tree
FullMod = glm(affordabilitty~A+AA+B+BB+BsmtFinType1+C+Condition1+D+E+Exterior1st+Exterior2nd+F+G+H+I+J+K+KitchenQual+L+M+MSSubClass+N+Neighborhood+O+P+Q+R+S+SaleCondition+SaleType+T+U+V+W+X+Y+Z,family=binomial(),data=btrain)
predictprobs = predict(FullMod,btest, type='response')
predictlogit = rep('Affordable',length(predictprobs))
predictlogit[predictprobs>0.5] = 'Unaffordable'
table(predictlogit,btest$affordabilitty)
mean(predictlogit==btest$affordabilitty)


backwardsSub = step(FullMod,trace=0) 
summary(backwardsSub)

BackSub = glm(affordabilitty ~ A + AA + B + BB + BsmtFinType1 + C + D + E + 
    H + I + K + KitchenQual + L + M + N + Neighborhood + O + 
    Q + S + SaleCondition + SaleType + T + V + W + Y + Z,family=binomial(),data=btrain)
predictprobs = predict(backwardsSub,btest, type='response')
predictlogit = rep('Affordable',length(predictprobs))
predictlogit[predictprobs>0.5] = 'Unaffordable'
table(predictlogit,btest$affordabilitty)
mean(predictlogit==btest$affordabilitty)


nothing = glm(affordabilitty~1,family=binomial(),data=btrain)
forwardsSub = step(nothing,scope=list(lower=formula(nothing),upper=formula(FullMod)),direction="forward",trace=0)

ForSub = glm(affordabilitty ~ A + Neighborhood + W + KitchenQual + S + BsmtFinType1 + 
    E + I + SaleCondition + L + Y + Q + M + N + H + SaleType + 
    D + T + BB + V + O + B + C,family=binomial(),data=btrain)

predictprobs = predict(forwardsSub,btest, type='response')
predictlogit = rep('Affordable',length(predictprobs))
predictlogit[predictprobs>0.5] = 'Unaffordable'
table(predictlogit,btest$affordabilitty)
mean(predictlogit==btest$affordabilitty)
```



Tree and Randomforest
```{r}
require(tree)
library(randomForest)  
library(e1071)  
library(caret)  
library(ggplot2)  
library(MASS)
library(ISLR)
library(rfUtilities)


set.seed(9999999)
n=dim(training1)[1]
traini = sample(1:n,n*0.9,replace=FALSE)
btrain = training1[traini,]
btest = training1[-traini,]

full=tree(affordabilitty~.,mindev=0,minsize=2,data=btrain)
pred.full=predict(full,newdata=btest,type="class")
table(pred.full,btest$affordabilitty)
mean(pred.full==btest$affordabilitty)

# Seeing which tree is best
cvtree = cv.tree(full,FUN=prune.misclass)
df = data.frame(size=cvtree$size,dev = cvtree$dev)
df=df[order(df$dev),]
df
pruned = prune.misclass(full,best=10)

#Finding Results of Pruned Tree
pred.pr = predict(pruned,newdata=btest,type="class")
table(pred.pr,btest$affordabilitty)
mean(pred.pr == btest$affordabilitty)


# Random Forest
rm1 = randomForest(affordabilitty~.,data=btrain,mtry=64,importance=TRUE,ntree=5000)
plot(rm1)
pred.rm=predict(rm1,data=btrain)
table(btrain$affordabilitty,pred.rm)
mean(btrain$affordabilitty==pred.rm)

pred.rm=predict(rm1,newdata=btest)
table(btest$affordabilitty,pred.rm)
mean(btest$affordabilitty==pred.rm)

df = importance(rm1)
df = as.data.frame(df)
df$rname = rownames(df)
df = arrange(df,desc(MeanDecreaseAccuracy))
df = df[,c(5,1,2,3,4)]
df1 = arrange(df,desc(MeanDecreaseGini))
varImpPlot(rm1,main="Variable Importance Plot")

# df of Mean Decrease Accuracy Best
MostImportantVar = c(df[1:12,1],df1[1:12,1])
x = which((table(MostImportantVar)==2)==TRUE)
x1 = names(x)
x1

btrain$BsmtFinType1
# Important Predictors
m2 = randomForest(affordabilitty~A+C+D+E+I+L+Neighborhood+O+P+S+U,data=btrain,mtry=sqrt(11),importance=TRUE,ntree=5000)

m3 = randomForest(affordabilitty~A+BB+BsmtFinType1+C+D+E+I+L+Neighborhood+S+W+Y,data=btrain,mtry=12,importance=TRUE,ntree=5000)

m4 = randomForest(affordabilitty~A+BB+D+E+I+L+Neighborhood+S+W+Y,data=btrain,mtry=10,importance=TRUE,ntree=5000)

m5 = randomForest(affordabilitty~A+BB+Neighborhood+W+Y,data=btrain,mtry=5,importance=TRUE,ntree=5000)

m6= randomForest(affordabilitty~A+B+BB+BsmtFinType1+C+D+E+I+J+L+Neighborhood+O+P+S+SaleCondition+T+W+X+Y,data=btrain,mtry=19,importance=TRUE,ntree=5000)

m7 =  randomForest(affordabilitty~A+BB+BsmtFinType1+C+D+E+I+L+Neighborhood+P+S+T+W+Y,data=btrain,mtry=14,importance=TRUE,ntree=5000)



#Predicting Using M2
pred.rm = predict(m2,data=btrain)
table(btrain$affordabilitty,pred.rm)
mean(btrain$affordabilitty==pred.rm)

pred.rm = predict(m2,newdata=btest)
table(btest$affordabilitty,pred.rm)
mean(btest$affordabilitty==pred.rm)

#Predicting Using M3 (Best Result 0.988)
pred.rm.t = predict(m3,newdata=btest)
table(btest$affordabilitty,pred.rm.t)
mean(btest$affordabilitty==pred.rm.t)

pred.rm.t1 = predict(m3,date=btrain)
mean(btrain$affordabilitty==pred.rm.t1)

# Predicting Using M4 (Best Result 0.988)
pred.rm.t = predict(m4,newdata=btest)
table(btest$affordabilitty,pred.rm.t)
mean(btest$affordabilitty==pred.rm.t)

pred.rm.t1 = predict(m4,data=btrain)
table(btrain$affordabilitty,pred.rm.t1)
mean(btrain$affordabilitty==pred.rm.t1)

# Predicting Using M5
pred.rm.t = predict(m5,newdata=btest)
table(btest$affordabilitty,pred.rm.t)
mean(btest$affordabilitty==pred.rm.t)

pred.rm.t1 = predict(m5,data=btrain)
table(btrain$affordabilitty,pred.rm.t1)
mean(btrain$affordabilitty==pred.rm.t1)

# Predicting Using M6
pred.rm.t = predict(m6,newdata=btest)
table(btest$affordabilitty,pred.rm.t)
mean(btest$affordabilitty==pred.rm.t)

pred.rm.t1 = predict(m5,data=btrain)
table(btrain$affordabilitty,pred.rm.t1)
mean(btrain$affordabilitty==pred.rm.t1)

# Predicting Using M7
pred.rm.t = predict(m7,newdata=btest)
table(btest$affordabilitty,pred.rm.t)
mean(btest$affordabilitty==pred.rm.t)

pred.rm.t1 = predict(m7,data=btrain)
table(btrain$affordabilitty,pred.rm.t1)
mean(btrain$affordabilitty==pred.rm.t1)

x = tuneRF(btrain[,colnames(btrain)!="affordabilitty"], btrain$affordabilitty, main="Mtry Plot")

library(ggpubr)
ggexport(plotlist = list(x),filename = "PCA.png")
```


TESTING # This code part wont work for you because it uses files that are in my laptop
```{r}
# Important Predictors
m2 = randomForest(affordabilitty~A+AA+B+BB+BsmtFinType1+C+D+E+Exterior2nd+H+I+J+L+N+Neighborhood+P+Q+R+S+T+U+W+X+Y+Z,data=training1,mtry=25,importance=TRUE,ntree=5000) 
m3 = randomForest(affordabilitty~.,data=training1,mtry=71,importance=TRUE,ntree=5000)
pred.rm = predict(m2,newdata=testing1)
pred.rm2 = predict(m3,newdata=testing1)


Attempt13 = data.frame(Ob=c(1:1500),affordabilitty=pred.rm)
Attempt13
write.csv(Attempt13,file="Attempt13.csv",row.names=FALSE)


# Reading in previous 
attempt5 = read.csv("Attempt5.csv")
attempt10 = read.csv("Attempt10.csv")


# Creating a dafa
df = data.frame(Attempt13,affordabilitty2 = attempt10$affordabilitty,affordabilitty3 = pred.rm2)


df$affordabilitty = as.factor(df$affordabilitty)
df$affordabilitty2 = as.factor(df$affordabilitty2)
df$affordabilitty3 = as.factor(df$affordabilitty3)

df$affordabilitty = revalue(df$affordabilitty,c("Unaffordable"="0","Affordable"="1"))
df$affordabilitty2 = revalue(df$affordabilitty2,c("Unaffordable"="0","Affordable"="1"))
df$affordabilitty3 = revalue(df$affordabilitty3,c("Unaffordable"="0","Affordable"="1"))

df$affordabilitty = as.numeric(as.character(df$affordabilitty))
df$affordabilitty2 = as.numeric(as.character(df$affordabilitty2))
df$affordabilitty3 = as.numeric(as.character(df$affordabilitty3))

df1 = df[,2:4]
for(i in 1:nrow(df1)){
  df1$Best[i] = names(sort(table(as.numeric(df1[i,1:3])),decreasing=TRUE))[1]
}

df$affordabilitty = as.factor(df$affordabilitty)
df$affordabilitty2 = as.factor(df$affordabilitty2)
df$affordabilitty3 = as.factor(df$affordabilitty3)
df$Best = as.factor(df$Best)

df$affordabilitty = revalue(df$affordabilitty ,c("0"="Unaffordable","1"="Affordable"))
df$affordabilitty2 = revalue(df$affordabilitty2,c("0"="Unaffordable","1"="Affordable"))
df$affordabilitty3 = revalue(df$affordabilitty3 ,c("0"="Unaffordable","1"="Affordable"))
df$Best= revalue(df$Best ,c("0"="Unaffordable","1"="Affordable"))

df[df$Best!=df$affordabilitty2,][,c(1,3,5)]

df[df$affordabilitty!=df$affordabilitty2,][,c(1,2,3)]
```


# Testing Part 2
```{r}
m3 = randomForest(affordabilitty~A+BB+BsmtFinType1+C+D+E+I+L+Neighborhood+S+W+Y,data=training1,mtry=12,importance=TRUE,ntree=5000)
pred.rm.t = predict(m3,newdata=testing1)

m7 =  randomForest(affordabilitty~A+BB+BsmtFinType1+C+D+E+I+L+Neighborhood+P+S+T+W+Y,data=training1,mtry=14,importance=TRUE,ntree=5000)
m8 = randomForest(affordabilitty~A+BB+BsmtFinType1+C+D+E+I+L+Neighborhood+P+S+T+W+Y+FireplaceQu+MSSubClass,data=training1,mtry=16,importance=TRUE,ntree=5000)
pred.rm.t = predict(m8,newdata=testing1)


Attempt5 = data.frame(Ob=c(1:1500),affordabilitty=pred.rm.t)

write.csv(Attempt5,file="Attempt5.csv",row.names=FALSE)

attempt10 = read.csv("Attempt10.csv")

df = data.frame(attempt10,affordability1 = pred.rm.t)
df
df[df$affordabilitty!=df$affordabilitty1,]
```


# Testing Part 3
```{r}
colnames(btrain)
mFull = randomForest(affordabilitty~.,data=btrain,mtry=15,importance=TRUE,ntree=770)
plot(mFull)
pred.rm.t = predict(mFull,newdata=btest)
table(btest$affordabilitty,pred.rm.t)
mean(btest$affordabilitty==pred.rm.t)


df = importance(mFull)
df = as.data.frame(df)
df$rname = rownames(df)
df = arrange(df,desc(MeanDecreaseAccuracy))
df = df[,c(5,1,2,3,4)]
df1 = arrange(df,desc(MeanDecreaseGini))
varImpPlot(rm1,main="Variable Importance Plot")
MostImportantVar = c(df[1:20,1],df1[1:20,1])
x = which((table(MostImportantVar)==2)==TRUE)
x1 = names(x)
x1


m2 = randomForest(affordabilitty~A+B+C+D+E+L+MSSubClass+Neighborhood+P+T+U, data=btrain,mtry=11,importance=TRUE,ntree=750)
plot(m2)
pred.rm.t = predict(m2,newdata=btest)
table(btest$affordabilitty,pred.rm.t)
mean(btest$affordabilitty==pred.rm.t)

```











