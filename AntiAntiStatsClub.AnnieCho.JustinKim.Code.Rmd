---
title: "AntiAntiStatsClub.AnnieCho.JustinKim.Code"
output: html_document
---

# Initial Attempt
Logistic Model Using Uncleaned Data
```{r}
library(leaps)
library(MASS)
library(mixlm)
library(plyr)
pdata <- read.csv(file = "HTrainLast.csv", stringsAsFactors = TRUE)
# Get rid of the observation column 
pdata = pdata[, -1]

# Create our training (90%) and validation (10%) set
set.seed(1234) # for reproducibility
traini = sample(1:3500,3500*0.9,replace=F)
ptrain = pdata[traini,]
ptest = pdata[-traini,]

# ??? apparently only predictors that we think are important
# We need to explore what the full logistic regression is like so we know which predictors are important
FullMod = glm(as.factor(affordabilitty)~(MSSubClass)+LotFrontage+LotArea+LandContour+LandSlope+BldgType+OverallQual+OverallCond+YearBuilt+YearRemodAdd+MasVnrArea+Foundation+TotalBsmtSF+X1stFlrSF+X2ndFlrSF+BsmtFullBath+BsmtHalfBath+FullBath+HalfBath+BedroomAbvGr+KitchenAbvGr+TotRmsAbvGrd+Functional+Fireplaces+GarageArea+OpenPorchSF+EnclosedPorch,family=binomial(),data=ptrain)

summary(FullMod)

predictprobs = predict(FullMod,ptest, type='response')
predictlogit = rep('1',length(predictprobs))
predictlogit[predictprobs>0.5] = '0'
confmatrix <- table(predictlogit,as.factor(ptest$affordabilitty))
confmatrix
ptest1 = as.factor(ptest$affordabilitty)
ptest1 = revalue(ptest1,c("Unaffordable"="0","Affordable"="1"))
mean(predictlogit==ptest1) #accuracy
```


# First Attempt at Data Cleaning
```{r}
library(reshape2)
library(ggplot2)
library(purrr)
library(tidyr)
library(corrplot)
library(plyr)
library(forcats) 
library(randomForest)
library(readr)
library(dplyr)


# Import training and testing data
training <- read.csv(file = "HTrainLast.csv", stringsAsFactors = TRUE)
real_testing <- read.csv(file = "HTestLastNoY.csv", stringsAsFactors = TRUE)
train1 = read.csv(file = "HTrainLast.csv", stringsAsFactors = TRUE)
real_testing$affordabilitty = rep(NA,nrow(real_testing))
train1 =  rbind(train1,real_testing)

# Combine training & testing
training = rbind(training,real_testing)

# Get rid of the observation column 
training = training[, -1]

# Manually changing predictors to factors based on Data Description
training$MSSubClass <- as.factor(training$MSSubClass)

# Changing all the NA's into "None" (another factor level)
#  Alley - it has 3273 NA's (no alley access)
training$Alley <- fct_explicit_na(training$Alley, na_level = "None")
# FireplaceQu = 1707 NA's (no fireplace)
training$FireplaceQu <- fct_explicit_na(training$FireplaceQu, na_level = "None")
# GarageType = 162 NA's (no garage)
training$GarageType <- fct_explicit_na(training$GarageType, na_level = "None")
# GarageFinish = 163 NA's (no garage)
training$GarageFinish <- fct_explicit_na(training$GarageFinish, na_level = "None")
# GarageQual = 163 NA's (no garage)                                           ~~~~ correlation w/ garage cond
training$Alley <- fct_explicit_na(training$Alley, na_level = "None")
# GarageCond = 163 NA's (no garage)
training$GarageCond <- fct_explicit_na(training$GarageCond, na_level = "None")

# PoolQC = 3490 NA's (no pool)
# PoolQC = 3490 NA's (no pool)
# Pools w/ missing PoolQC, even though they have a pool
training[(training$PoolArea > 0) & is.na(training$PoolQC),c('PoolQC','PoolArea')]
# Calculate the mean pool size for each level of pool condition
training[,c('PoolQC','PoolArea')] %>% group_by(PoolQC) %>%
  summarise(mean = mean(PoolArea), counts = n()) 
# Assign these pools a quality based on the mean of that size of a pool 
training[644,'PoolQC'] = 'Fa' #561
training[1052,'PoolQC'] = 'Ex' #368
training[1192,'PoolQC'] = 'Fa' #561
training[1425,'PoolQC'] = 'Ex' #444
training[4555,'PoolQC'] = 'Fa' #561
training[4668,'PoolQC'] = 'Fa' #561
# Assign the rest of the PoolQC that have a PoolArea=0, to none
training$PoolQC <- fct_explicit_na(training$PoolQC, na_level = "None")


training$Fence <- fct_explicit_na(training$Fence, na_level = "None")
# BsmtQual
training$BsmtQual <- fct_explicit_na(training$BsmtQual, na_level = "None")
# BsmtCond
training$BsmtCond <- fct_explicit_na(training$BsmtCond, na_level = "None")
# Bsmt Exposure
training$BsmtExposure <- fct_explicit_na(training$BsmtExposure, na_level = "None")
# Bsmt Fintype1
training$BsmtFinType1 <- fct_explicit_na(training$BsmtFinType1, na_level = "None")
# Bsmt Fintype2
training$BsmtFinType2 <- fct_explicit_na(training$BsmtFinType2, na_level = "None")


#training$GarageYrBlt = as.numeric(as.character(training$GarageYrBlt))
#training$GarageYrBlt[training$GarageYrBlt=="2207"] = "None"
# We can see that GarageType, GarageYrBlt, GarageFinish, GarageCars, GarageArea, GarageQual, GarageCond -> means no garage
#training[which(is.na(training$GarageYrBlt)),c(58,59,60,61,62,63,64)]
#training$GarageYrBlt[is.na(training$GarageYrBlt)] = 0
#training$GarageCars[is.na(training$GarageCars)] = 0
#training$GarageArea[is.na(training$GarageArea)] = 0
#training$GarageQual <- fct_explicit_na(training$GarageQual, na_level = "None")

training=training[,-c(9,74,75)] # Get rid of utilities, MiscFeature, MiscVal

# Getting only the data without NA's
NoNaData = training[,colSums(is.na(training))==0]
NoNaNumeric = select_if(NoNaData, is.numeric)
NoNaCat = select_if(NoNaData, is.factor)

# Replacing MSZoning NA's
MSdata = cbind(MSZoning=training$MSZoning,NoNaData)
MStrain = MSdata[complete.cases(MSdata),]
MStest = MSdata[is.na(MSdata),]
rfMS = randomForest(MSZoning~.,data=MStrain,mtry=54,importance=TRUE)
pred.MS = predict(rfMS,newdata=MStest)
training$MSZoning[is.na(training$MSZoning)] = pred.MS

# Replacing LotFrontage NA
library(gbm)
Lotdata = cbind(LotFrontage=training$LotFrontage,NoNaData)
Lottrain = Lotdata[complete.cases(Lotdata),]
Lottest = Lotdata[is.na(Lotdata),]
n = nrow(Lottrain)
traini = sample(1:n,2930,replace=FALSE)
Lottn = Lottrain[traini,]
Lottt = Lottrain[-traini,]
Lotgbm = gbm(LotFrontage~.,data=Lottrain,distribution="gaussian",n.tree=5000,interaction.depth=4)
pred.Lot = predict(Lotgbm,newdata=Lottest,n.tree=5000)
training$LotFrontage[is.na(training$LotFrontage)] = pred.Lot

# Replacing Exteriro 1st with sample as its only 1 NA
training$Exterior1st[is.na(training$Exterior1st)] = sample(levels(training$Exterior1st),sum(is.na(training$Exterior1st),na.rm=TRUE),replace=TRUE)

# Replacing Exterior 2nd with sample as its only 1 NA
training$Exterior2nd[is.na(training$Exterior2nd)] = sample(levels(training$Exterior2nd),sum(is.na(training$Exterior2nd),na.rm=TRUE),replace=TRUE)


# Replacing NA for MasVnrArea and MasVnrType
training$MasVnrType <- fct_explicit_na(training$MasVnrType, na_level = "None")
training$MasVnrArea[is.na(training$MasVnrArea)] = 0
levels(training$MasVnrType) = c(levels(training$MasVnrType),"Other")
training$MasVnrType[training$MasVnrType=="None" & training$MasVnrArea!=0] = "Other"


# Making BsmtFinSF1 NA to 0
training$BsmtFinSF1[is.na(training$BsmtFinSF1)] = 0

# Making BsmtFinSF2 NA to 0
training$BsmtFinSF2[is.na(training$BsmtFinSF2)] = 0

# Making BsmtUnfSF NA to 0
training$BsmtUnfSF[is.na(training$BsmtUnfSF)] = 0

# Making BsmtFinSF2 NA to 0
training$TotalBsmtSF[is.na(training$TotalBsmtSF)] = 0

# Sampling to fill NA of Electircal
training$Electrical[is.na(training$Electrical)] = "SBrkr"

# Basement Full Bath
training[is.na(training$BsmtFullBath),]
training$BsmtFullBath[is.na(training$BsmtFullBath)] = 0


# Basement Half Bath
training[is.na(training$BsmtHalfBath),]
training$BsmtHalfBath[is.na(training$BsmtHalfBath)] = 0

# Kitchen Qual
training[is.na(training$KitchenQual),] 
training$KitchenQual[is.na(training$KitchenQual)] = "TA"

# Functional
training[is.na(training$Functional),] 
training$Functional[is.na(training$Functional)] = "Typ"

# Garage Year Built
training$GarageYrBlt[is.na(training$GarageYrBlt)] = 2018
training$GarageYrBlt[training$GarageYrBlt==2018] = training$YearBuilt[training$GarageYrBlt==2018]
training$GarageYrBlt[training$GarageYrBlt==2207] = training$YearBuilt[training$GarageYrBlt==2207]

# Garage Cars
training$GarageCars[is.na(training$GarageCars)] = 0

# Garage Area
training$GarageArea[is.na(training$GarageArea)] = 0

# Garage Qual
training$GarageQual <- fct_explicit_na(training$GarageQual, na_level = "None")

# Sale Type: the only missing data is normal
training[is.na(training$SaleType),c('SaleCondition')]
# Noraml is most associated with "WD"
table(training$SaleCondition, training$SaleType)
# Replace all NA sale type w/ "WD"
training$SaleType[is.na(training$SaleType)] = 'WD'



# House Age And Yes/No for remodeled
# Remodeled (Categorical)
training$Remodeled <- rep("Yes", nrow(training))
training$Remodeled[which (training$YearRemodAdd - training$YearBuilt==0)]<- "No"
training$Remodeled <- as.factor(training$Remodeled)
# House Age
# age = 2018 - yearbuilt (if not remodeled)
training$HouseAge[which(training$Remodeled == "No")] <- 2018 - training$YearBuilt[which(training$Remodeled == "No")]
# age = 2018 - yearRemodeled (if remodeled)
training$HouseAge[which(training$Remodeled == "Yes")] <- 2018 - training$YearRemodAdd[which(training$Remodeled == "Yes")]
# Condense levels of LotShape
training$LotShape <- as.character(training$LotShape)
training$LotShape[which (training$LotShape!= "Reg")] <- "IR"
training$LotShape <- as.factor(training$LotShape)

# Taking out BsmtFinSF1 and 2
training$BsmtTotalFin = training$BsmtFinSF1 + training$BsmtFinSF2

training = training[,colnames(training) != "BsmtFinSF2"]
training = training[,colnames(training) != "BsmtFinSF1"]


# Splitting training by numeric and factors
training.numeric <- select_if(training, is.numeric)
full.numeric = select_if(training, is.numeric)
training.factors <- select_if(training, is.factor)
full.factors =select_if(training, is.factor) 


################################# 
library(corrplot)
corr <- round(cor(training.numeric),3)
corrplot.mixed(corr, lower = "number", upper = "pie")
# See which predictors have a correlation equal to or higher than 0.7
corr[which(abs(corr) >=0.7 & abs(corr) !=1 )]
which(abs(corr) >=0.7 & abs(corr) !=1 )
temp_ind <- which(abs(corr) >=0.7 & abs(corr) !=1  , arr.ind = TRUE)
corr_table <- data.frame(predictors = paste(rownames(corr)[temp_ind[,"row"]],  colnames(corr)[temp_ind[,"col"]], sep=', '), values = corr[which(abs(corr) >=0.7 & abs(corr) !=1 )])

training.numeric = training.numeric[,colnames(training.numeric) != "YearBuilt"]
training.numeric = training.numeric[,colnames(training.numeric) != "YearRemodAdd"]
training.numeric = training.numeric[,colnames(training.numeric) != "X1stFlrSF"]
training.numeric = training.numeric[,colnames(training.numeric) != "TotRmsAbvGrd"]
training.numeric = training.numeric[,colnames(training.numeric) != "GarageCars"]
# Adding Affordability to Numeric Variables 
pca = princomp(training.numeric,cor=T)

x = summary(pca)
pc.comp <- pca$scores
pcdf = pc.comp[,1:22]
pcdf = data.frame(pcdf)
colnames(pcdf) = c(LETTERS[1:22])

training.factors = cbind(training.factors,pcdf)
training.factors = training.factors[,colnames(training.factors) != "Exterior2nd"]

testing1 = training.factors[3501:5000,]
training1 = training.factors[1:3500,]
training1 = training1[complete.cases(training1),]
```


Splitting the training to 90% and 10%
```{r}
set.seed(9999999)
n=dim(training1)[1]
traini = sample(1:n,n*0.9,replace=FALSE)
btrain = training1[traini,]
btest = training1[-traini,]
```

Logistic Model With all predictors included using Cleaned Data
```{r}
m2 = glm(affordabilitty~.,family=binomial(),data=btrain)


predictprobs = predict(m2,btest, type='response')
predictlogit = rep('Affordable',length(predictprobs))
predictlogit[predictprobs>0.5] = 'Unaffordable'
table(predictlogit,btest$affordabilitty)
mean(predictlogit==btest$affordabilitty)
```


Logistic With Backward And Forward Selection
```{r}
library(leaps)
library(MASS)
library(mixlm)
library(plyr)

# Using variables that came out important from tree
FullMod = glm(affordabilitty~.,family=binomial(),data=btrain)
predictprobs = predict(FullMod,btest, type='response')
predictlogit = rep('Affordable',length(predictprobs))
predictlogit[predictprobs>0.5] = 'Unaffordable'
table(predictlogit,btest$affordabilitty)
mean(predictlogit==btest$affordabilitty)

# Takes long to run so put the final result as BackSub in the next line
# backwardsSub = step(FullMod,trace=0) 
#summary(backwardsSub)

BackSub = glm(affordabilitty ~ MSSubClass + MSZoning + Street + LotShape + 
    LandContour + LotConfig + LandSlope + Neighborhood + Condition1 + 
    HouseStyle + RoofStyle + RoofMatl + MasVnrType + BsmtQual + 
    BsmtCond + BsmtExposure + BsmtFinType1 + HeatingQC + Electrical + 
    KitchenQual + Functional + FireplaceQu + GarageType + GarageFinish + 
    PavedDrive + PoolQC + Fence + SaleType + SaleCondition + 
    Remodeled + A + B + C + D + E + H + I + J + K + L + M + N + 
    O + P + R + S + U + V,family=binomial(),data=btrain)
predictprobs = predict(BackSub,btest, type='response')
predictlogit = rep('Affordable',length(predictprobs))
predictlogit[predictprobs>0.5] = 'Unaffordable'
table(predictlogit,btest$affordabilitty)
mean(predictlogit==btest$affordabilitty)

# Takes logn to run so put the final result as ForSub in the next line
#nothing = glm(affordabilitty~1,family=binomial(),data=btrain)
#forwardsSub = step(nothing,scope=list(lower=formula(nothing),upper=formula(FullMod)),direction="forward",trace=0)
ForSub = glm(affordabilitty ~ A + Neighborhood + KitchenQual + SaleCondition + 
    S + BsmtExposure + I + MSZoning + SaleType + H + Functional + 
    E + D + ExterQual + L + ExterCond + P + J + Street + M + 
    LotShape + T + BsmtQual + C + G + Remodeled + HeatingQC + 
    GarageType + BsmtCond + Foundation + BsmtFinType1 + R,family=binomial(),data=btrain)

predictprobs = predict(ForSub,btest, type='response')
predictlogit = rep('Affordable',length(predictprobs))
predictlogit[predictprobs>0.5] = 'Unaffordable'
table(predictlogit,btest$affordabilitty)
mean(predictlogit==btest$affordabilitty)
```



Tree and Randomforest
```{r}
require(tree)
library(randomForest)  
library(e1071)  
library(caret)  
library(ggplot2)  
library(MASS)
library(ISLR)
library(rfUtilities)


set.seed(9999999)
n=dim(training1)[1]
traini = sample(1:n,n*0.9,replace=FALSE)
btrain = training1[traini,]
btest = training1[-traini,]

full=tree(affordabilitty~.,mindev=0,minsize=2,data=btrain)
pred.full=predict(full,newdata=btest,type="class")
table(pred.full,btest$affordabilitty)
mean(pred.full==btest$affordabilitty)

# Seeing which tree is best
cvtree = cv.tree(full,FUN=prune.misclass)
df = data.frame(size=cvtree$size,dev = cvtree$dev)
df=df[order(df$dev),]
df
pruned = prune.misclass(full,best=83)

#Finding Results of Pruned Tree
pred.pr = predict(pruned,newdata=btest,type="class")
table(pred.pr,btest$affordabilitty)
mean(pred.pr == btest$affordabilitty)


# Bagging
rm1 = randomForest(affordabilitty~.,data=btrain,mtry=64,importance=TRUE,ntree=2000)
jpeg('baggingmodel.jpg')
plot(rm1,main="Bagging Model Plot")
dev.off()
pred.rm=predict(rm1,data=btrain)
table(btrain$affordabilitty,pred.rm)
mean(btrain$affordabilitty==pred.rm)

pred.rm=predict(rm1,newdata=btest)
table(btest$affordabilitty,pred.rm)
mean(btest$affordabilitty==pred.rm)

df = importance(rm1)
df = as.data.frame(df)
df$rname = rownames(df)
df = arrange(df,desc(MeanDecreaseAccuracy))
df = df[,c(5,1,2,3,4)]
df1 = arrange(df,desc(MeanDecreaseGini))

# df of Mean Decrease Accuracy Best
MostImportantVar = c(df[1:9,1],df1[1:9,1])
x = which((table(MostImportantVar)==2)==TRUE)
x1 = names(x)
x1

m2 = randomForest(affordabilitty~A+C+D+E+I+L+Neighborhood+P+S+T,data=btrain,mtry=sqrt(10),importance=TRUE,ntree=1300)
plot(m2)

m3 = randomForest(affordabilitty~A+C+D+E+Exterior1st+H+I+L+Neighborhood+P+S+T+U,data=btrain,mtry=sqrt(13),importance=TRUE,ntree=2500)
plot(m3)

m4 = randomForest(affordabilitty~A+C+D+E+I+L+Neighborhood,data=btrain,mtry=sqrt(7),importance=TRUE,ntree=2200)
plot(m4)



#Predicting Using M2
pred.rm = predict(m2,data=btrain)
table(btrain$affordabilitty,pred.rm)
mean(btrain$affordabilitty==pred.rm)

pred.rm = predict(m2,newdata=btest)
table(btest$affordabilitty,pred.rm)
mean(btest$affordabilitty==pred.rm)

#Predicting Using M3
pred.rm.t1 = predict(m3,date=btrain)
mean(btrain$affordabilitty==pred.rm.t1)

pred.rm.t = predict(m3,newdata=btest)
table(btest$affordabilitty,pred.rm.t)
mean(btest$affordabilitty==pred.rm.t)

#Predicting Using M4
pred.rm.t1 = predict(m4,date=btrain)
mean(btrain$affordabilitty==pred.rm.t1)

pred.rm.t = predict(m4,newdata=btest)
table(btest$affordabilitty,pred.rm.t)
mean(btest$affordabilitty==pred.rm.t)

dfMultiple = data.frame(A=rep(NA,nrow(btest)), B = rep(NA,nrow(btest)), C = rep(NA,nrow(btest)), D = rep(NA,nrow(btest)),E = rep(NA,nrow(btest)), F= rep(NA,nrow(btest)), G = rep(NA,nrow(btest)), H = rep(NA,nrow(btest)), I=rep(NA,nrow(btest)),J=rep(NA,nrow(btest)) )

for(i in 1:10){
  m2 = randomForest(affordabilitty~A+C+D+E+I+L+Neighborhood+P+S+T,data=btrain,mtry=sqrt(i),importance=TRUE,ntree=1300)
  dfMultiple[,i] = predict(m2,newdata=btest)
  dfMultiple[,i] = as.factor(dfMultiple[,i])
  dfMultiple[,i] = revalue(dfMultiple[,i],c("Unaffordable"="0","Affordable"="1"))
  dfMultiple[,i] = as.numeric(as.character(dfMultiple[,i]))
}


for(i in 1:nrow(dfMultiple)){
  dfMultiple$Best[i] = names(sort(table(as.numeric(dfMultiple[i,1:10])),decreasing=TRUE))[1]
}

dfMultiple$Best = as.factor(dfMultiple$Best)
dfMultiple$Best = revalue(dfMultiple$Best,c("0"="Unaffordable","1"="Affordable"))

table(btest$affordabilitty,dfMultiple$Best)
mean(btest$affordabilitty==dfMultiple$Best)

```


TESTING
```{r}
#Predicting Using M2
m2 = randomForest(affordabilitty~A+C+D+E+I+L+Neighborhood+P+S+T,data=training1,mtry=sqrt(10),importance=TRUE,ntree=1100)

dfMultiple = data.frame(A=rep(NA,nrow(testing1)), B = rep(NA,nrow(testing1)), C = rep(NA,nrow(testing1)), D = rep(NA,nrow(testing1)),E = rep(NA,nrow(testing1)), F= rep(NA,nrow(testing1)), G = rep(NA,nrow(testing1)), H = rep(NA,nrow(testing1)), I=rep(NA,nrow(testing1)),J=rep(NA,nrow(testing1)) )


for(i in 1:10){
  m2 = randomForest(affordabilitty~A+C+D+E+I+L+Neighborhood+P+S+T,data=training1,mtry=sqrt(i),importance=TRUE,ntree=1100)
  dfMultiple[,i] = predict(m2,newdata=testing1)
  dfMultiple[,i] = as.factor(dfMultiple[,i])
  dfMultiple[,i] = revalue(dfMultiple[,i],c("Unaffordable"="0","Affordable"="1"))
  dfMultiple[,i] = as.numeric(as.character(dfMultiple[,i]))
}

for(i in 1:nrow(dfMultiple)){
  dfMultiple$Best[i] = names(sort(table(as.numeric(dfMultiple[i,1:10])),decreasing=TRUE))[1]
}

dfMultiple$Best = as.factor(dfMultiple$Best)
dfMultiple$Best = revalue(dfMultiple$Best,c("0"="Unaffordable","1"="Affordable")) # Prediction for actual testing data using training dataset
```




# Updated Data Cleaning and Methods Based on Updated Data Cleaning
```{r}
library(reshape2)
library(ggplot2)
library(purrr)
library(tidyr)
library(corrplot)
library(plyr)
library(forcats) 
library(randomForest)
library(readr)
library(dplyr)
library(ggpubr)

# Import training and testing data
real_training <- read.csv(file = "HTrainLast.csv", stringsAsFactors = TRUE)
real_testing <- read.csv(file = "HTestLastNoY.csv", stringsAsFactors = TRUE)
real_testing$affordabilitty = rep(NA,nrow(real_testing))

# Combine training & testing
combined = rbind(real_training,real_testing)

# Get rid of the observation column 
combined = combined[, -1]

summary(combined)

# Manually changing predictors to factors based on Data Description
combined$MSSubClass <- as.factor(combined$MSSubClass)


# Get rid of utilities, MiscFeature, MiscVal
which(names(combined ) =='Utilities' | names(combined ) =='MiscFeature' |  names(combined ) =='MiscVal')
combined=combined[,-c(9,74,75)] 

###### Dealing with Missing Data

# Changing all the NA's into "None" (another factor level)
#  Alley - 4662 NA's (no alley access)
combined$Alley <- fct_explicit_na(combined$Alley, na_level = "None")
# BsmtQual - 140 NA's (no basement)
combined$BsmtQual <- fct_explicit_na(combined$BsmtQual, na_level = "None")
# BsmtCond - 137 NA's (no basement)
combined$BsmtCond <- fct_explicit_na(combined$BsmtCond, na_level = "None")
# Bsmt Exposure - 137 NA's (no basement)
combined$BsmtExposure <- fct_explicit_na(combined$BsmtExposure, na_level = "None")
# Bsmt Fintype1 - 134 NA's (no basement)
combined$BsmtFinType1 <- fct_explicit_na(combined$BsmtFinType1, na_level = "None")
# Bsmt Fintype2 - 135 NA's (no basement)
combined$BsmtFinType2 <- fct_explicit_na(combined$BsmtFinType2, na_level = "None")
# FireplaceQu - 2455 NA's (no fireplace)
combined$FireplaceQu <- fct_explicit_na(combined$FireplaceQu, na_level = "None")
# Fence - 3988 NA's (no fence)
combined$Fence <- fct_explicit_na(combined$Fence, na_level = "None")

# CATEGORICAL
# If NA's in GarageType, GarageFinish, GarageQual, GarageCond all mean no garage, they should have the same number of missing values, we investigate further
combined[(!is.na(combined$GarageType) & is.na(combined$GarageFinish)),c('GarageType','GarageFinish', 'GarageCars', 'GarageArea','GarageCond','GarageQual')]
# We see that there are two observations, that were mislabeled as having detached garages, when in fact GarageFinish, GarageCars, GarageArea, GarageCond, GarageQual indicate that a garage doesn't exist
# GarageType - 252 NA's (no garage)
combined$GarageType[c(833, 4009)] <- NA # change the 2 outliers in GarageType to NA
combined$GarageType <- fct_explicit_na(combined$GarageType, na_level = "None")
# GarageFinish - 254 NA's (no garage)
combined$GarageFinish <- fct_explicit_na(combined$GarageFinish, na_level = "None")
# GarageQual - 254 NA's (no garage)                                      
combined$GarageQual <- fct_explicit_na(combined$GarageQual, na_level = "None")
# GarageCond - 254 NA's (no garage)
combined$GarageCond <- fct_explicit_na(combined$GarageCond, na_level = "None")
# Garage Year Built - 254 NA's
# NA for GarageYrBlt shows "none" for GarageType, GarageFinish, GarageCars, GarageArea, GarageQual, GarageCond -> means no garage
combined[which(is.na(combined$GarageYrBlt)),c('GarageYrBlt','GarageType', 'GarageFinish', 'GarageCars', 'GarageArea','GarageQual','GarageCond')]
combined$GarageYrBlt[is.na(combined$GarageYrBlt)] = 2018
# OUTLIER: since we saw that for GarageYrBlt max = 2207
combined[which(combined$GarageYrBlt > 2018),c('GarageYrBlt','YearBuilt','GarageType', 'GarageFinish', 'GarageCars', 'GarageArea','GarageQual','GarageCond')]
# Can see these outlier visually
garageyrblt_outlier <- ggplot(data = combined, aes(x = GarageQual, y = GarageYrBlt)) +  geom_boxplot(aes(fill = GarageQual), show.legend = FALSE)
# We know that there's one outlier, and it has a garage, but the GarageYrBlt is wrong, so we just assign GarageYrBlt = YearBuilt
combined$GarageYrBlt[combined$GarageYrBlt==2207] = combined$YearBuilt[combined$GarageYrBlt==2207]
# Visually see impact of removing outlier 
garageyrblt_outlier_after <- ggplot(data = combined, aes(x = GarageQual, y = GarageYrBlt)) +  geom_boxplot(aes(fill = GarageQual), show.legend = FALSE)
# Creating figure 1
figure <- ggarrange(garageyrblt_outlier, garageyrblt_outlier_after, 
          labels = c("A", "B"),
          ncol = 2, nrow = 1)

garageyrblt<- annotate_figure(figure,
                top = text_grob("Comparison of Garage Year Built Across Garage Quality", face = "bold", size = 12, hjust = 0.5),
                bottom = text_grob("Figure 1A: Identifying the outlier in GarageYrBlt. The outlier observation has a GarageQual ‘TA’, meaning  \n that it has a garage on the property. We set the GarageYrBlt value equal to the YearBuilt value. \n \n Figure 1B: Reassigning our outlier positively impacted our predictor GarageYrBlt.",hjust = 0, x = 0, face = "italic", size = 10),
                fig.lab = "Figure 1", fig.lab.face = "bold"
                )
garageyrblt
ggexport(plotlist = list(garageyrblt),filename = "garageyrblt.png")
rm(list = c("garageyrblt_outlier", "garageyrblt_outlier_after", "figure", "garageyrblt"))

# PoolQC = 3490 NA's (no pool)
# Although all PoolQC with NA's are supposed to mean that they don't have a pool, we found 6 cases, where PoolQC = NA, even though they have a pool area
combined[(combined$PoolArea > 0) & is.na(combined$PoolQC),c('PoolQC','PoolArea')]
# Can see these outliers visually 
poolqc_outlier <- ggplot(data = combined, aes(x = PoolQC, y = PoolArea)) +  geom_boxplot(aes(fill = PoolQC), show.legend = FALSE) +  scale_fill_brewer(palette="Dark2")
# Calculate the mean pool size for each level of pool condition
combined[,c('PoolQC','PoolArea')] %>% group_by(PoolQC) %>%
  summarise(mean = mean(PoolArea), counts = n()) 
# Assign these pools a quality based on the mean of that size of a pool 
combined[644,'PoolQC'] = 'Fa' #561
combined[1052,'PoolQC'] = 'Ex' #368
combined[1192,'PoolQC'] = 'Fa' #561
combined[1425,'PoolQC'] = 'Ex' #444
combined[4555,'PoolQC'] = 'Fa' #561
combined[4668,'PoolQC'] = 'Fa' #561
# Assign the rest of the PoolQC that have a PoolArea=0, to none
combined$PoolQC <- fct_explicit_na(combined$PoolQC, na_level = "None")
# Visually see impact of removing outlier 
poolqc_outlier_after <- ggplot(data = combined, aes(x = PoolQC, y = PoolArea)) +  geom_boxplot(aes(fill = PoolQC), show.legend = FALSE) +  scale_fill_brewer(palette="Dark2")
# Creating Figure 2
figure <- ggarrange(poolqc_outlier, poolqc_outlier_after, 
          labels = c("A", "B"),
          ncol = 2, nrow = 1)
poolqc<- annotate_figure(figure,
                top = text_grob("Comparison of Pool Area Across Pool Quality", face = "bold", size = 12, hjust = 0.5),
                bottom = text_grob("Figure 2A: Identifying the outliers in PoolQC. The 6 outlier observations have no pool, but 3 unique Pool \n Area values. \n \n Figure 2B: Relabeling our outliers positively impacted our predictor PoolQC.",hjust = 0, x = 0, face = "italic", size = 10),
                fig.lab = "Figure 2", fig.lab.face = "bold"
                )
poolqc
ggexport(plotlist = list(poolqc),filename = "poolqc.png")
rm(list = c("poolqc_outlier", "poolqc_outlier_after", "figure", "poolqc"))

# 1st approach to MasVnrType and MasVnrArea
# Replacing MasVnrType NA's
#MasVnrdata = cbind(MasVnrType=training$MasVnrType,NoNaData)
#MasVnrtrain = MasVnrdata[complete.cases(MasVnrdata),]
#MasVnrtest = MasVnrdata[is.na(MasVnrdata),]
#n = nrow(MasVnrtrain)
#traini = sample(1:n,3466,replace=FALSE)
#Mastn = MasVnrtrain[traini,]
#Mastt = MasVnrtrain[-traini,]
#rfMasVnr = randomForest(MasVnrType~.,data=MasVnrtrain,mtry=54,importance=TRUE) #Creating Model to replace NA
#pred.Vnr = predict(rfMasVnr,newdata=MasVnrtest)
#training$MasVnrType[is.na(training$MasVnrType)] = pred.Vnr #Changing NA to predicted 

# Replacing MasVnrArea 
#library(gbm)

#Masdata = cbind(MasVnrArea=training$MasVnrArea,NoNaData)
#Mastrain = Masdata[complete.cases(Masdata),]
#Mastest = Masdata[is.na(Masdata),]
#n = nrow(Mastrain)
#traini = sample(1:n,3468,replace=FALSE)
#Mastn = Mastrain[traini,]
#Mastt = Mastrain[-traini,]
#Masgbm = gbm(MasVnrArea~.,data=Mastrain,distribution="gaussian",n.tree=5000,interaction.depth=4)
#pred.Mas = predict(Masgbm,newdata=Mastest,n.tree=5000)
#training$MasVnrArea[is.na(training$MasVnrArea)] = pred.Mas

# 2nd Attempt for MasVnrArea and MasVnrType
# Replacing NA for MasVnrArea and MasVnrType
# MasVnrType - 49 NA's (no masonry)   
combined$MasVnrType <- fct_explicit_na(combined$MasVnrType, na_level = "None")
# MasVnrArea - 46 NA's (no masonry area)  
combined$MasVnrArea[is.na(combined$MasVnrArea)] = 0
# There's many instances, where MasVnrType is None, but it has a MasVnrArea, so we set these MasVnrType = "Other"
# Visual representation of outliers
masvnrtype_outlier <- ggplot(data = combined, aes(x = MasVnrType, y = MasVnrArea)) +  geom_boxplot(aes(fill = MasVnrType), show.legend = FALSE) +  scale_fill_brewer(palette="YlOrRd")
# Outputting the 16 outlier cases
combined[(combined$MasVnrArea > 0) &(combined$MasVnrType =="None"), c("MasVnrType","MasVnrArea")]
# The ones where MasVnrArea = 1, must be an error, so we set it equal to 0.
combined$MasVnrArea[(combined$MasVnrArea == 1)] = 0
# Add a level to MasVnrType: "Other"
levels(combined$MasVnrType) = c(levels(combined$MasVnrType),"Other")
combined$MasVnrType[combined$MasVnrType=="None" & combined$MasVnrArea!=0] = "Other"
# Visually see impact of removing outlier 
masvnrtype_outlier_after <- ggplot(data = combined, aes(x = MasVnrType, y = MasVnrArea)) +  geom_boxplot(aes(fill = MasVnrType), show.legend = FALSE) + scale_fill_brewer(palette="YlOrRd")
# Creating Figure 3
figure <- ggarrange(masvnrtype_outlier, masvnrtype_outlier_after, 
          labels = c("A", "B"),
          ncol = 2, nrow = 1)
masvnrtype <- annotate_figure(figure,
                top = text_grob("Comparison of Masonry Veneer Area Across Type", face = "bold", size = 12, hjust = 0.5),
                bottom = text_grob("Figure 3A: Identifying the outliers in MasVnrType. The 16 outlier observations are labeled in the ‘None’ \n level of MasVnrType. This indicates, they have no Masonry Veneer, but they have a corresponding \n Masonry Veneer Area. We reclassify them as “Other”. \n \n Figure 3B: Relabeling our outliers positively impacted our predictor MasVnrType. The classification of a \n lack of Masonry Veneer is consistent between MasVnrArea and MasVnrType.",hjust = 0, x = 0, face = "italic", size = 10),
                fig.lab = "Figure 3", fig.lab.face = "bold"
                )
masvnrtype
ggexport(plotlist = list(masvnrtype),filename = "masvnrtype.png")
rm(list = c("masvnrtype_outlier", "masvnrtype_outlier_after", "figure", "masvnrtype"))

# NUMERICAL
# For Basement, we see that the same 2 observations are missing, and see that their BmtQual = None, which means that they have no basement, so we set the following equal to 0
combined[is.na(combined$BsmtFinSF1), 29:37]
# BsmtFinSF1 - 2 NA's
combined$BsmtFinSF1[is.na(combined$BsmtFinSF1)] = 0
# BsmtFinSF2 - 2 NA's
combined$BsmtFinSF2[is.na(combined$BsmtFinSF2)] = 0
# BsmtUnfSF - 2 NA's
combined$BsmtUnfSF[is.na(combined$BsmtUnfSF)] = 0
# TotalBsmtSF - 2 NA's
combined$TotalBsmtSF[is.na(combined$TotalBsmtSF)] = 0
# For Basement Bathrooms, we see that the same 4 observations are missing, and see that their BmtQual = None, which means that they have no basement, so we set them to 0
which(is.na(combined$BsmtFullBath)) == which(is.na(combined$BsmtHalfBath))
combined[is.na(combined$BsmtHalfBath),c('BsmtHalfBath','BsmtFullBath','BsmtQual')] 
# Basement Full Bath - 4 NA's
combined$BsmtFullBath[is.na(combined$BsmtFullBath)] = 0
# Basement Half Bath - 4 NA's
combined$BsmtHalfBath[is.na(combined$BsmtHalfBath)] = 0
# NA for GarageCars & GarageArea shows "none" for GarageType, GarageFinish, GarageCars, GarageArea, GarageQual, GarageCond -> means no garage
combined[(is.na(combined$GarageCars) & is.na(combined$GarageArea)),c('GarageType','GarageFinish', 'GarageCars', 'GarageArea','GarageCond','GarageQual')]
# Garage Cars - 2 NA's
combined$GarageCars[is.na(combined$GarageCars)] = 0
# Garage Area - 2 NA's
combined$GarageArea[is.na(combined$GarageArea)] = 0

# Values that can't be relabeled as "None"/ 0 , we have to sample
# First attempt: sample everything
# MSZoning - 5 NA's
#combined$MSZoning[is.na(combined$MSZoning)] = sample(levels(combined$MSZoning),sum(is.na(combined$MSZoning),na.rm=TRUE),replace=TRUE)
# LotFrontage - 815 NA's 
#combined$LotFrontage[is.na(combined$LotFrontage)] = median(combined$LotFrontage,na.rm=TRUE)
# Electrical - 2 NA's
#combined$Electrical[is.na(combined$Electrical)] = sample(levels(combined$Electrical),sum(is.na(combined$Electrical),na.rm=TRUE),replace=TRUE)
# KitchenQual - 2 NA's
#combined$KitchenQual[is.na(combined$KitchenQual)] = sample(levels(combined$KitchenQual),sum(is.na(combined$KitchenQual),na.rm=TRUE),replace=TRUE)
# Functional - 3 NA's
#combined$Functional[is.na(combined$Functional)] = sample(levels(combined$Functional),sum(is.na(combined$Functional),na.rm=TRUE),replace=TRUE)
# Exterior 1st - 1 NA
#combined$Exterior1st[is.na(combined$Exterior1st)] = sample(levels(combined$Exterior1st),sum(is.na(commbined$Exterior1st),na.rm=TRUE),replace=TRUE)
# Exterior 2nd - 1 NA
#combined$Exterior2nd[is.na(combined$Exterior2nd)] = sample(levels(combined$Exterior2nd),sum(is.na(combined$Exterior2nd),na.rm=TRUE),replace=TRUE)
# Sale Type - 1 NA
#combined$SaleType[is.na(combined$Exterior2nd)] = sample(levels(combined$Exterior2nd),sum(is.na(combined$Exterior2nd),na.rm=TRUE),replace=TRUE)

# Second attempt:
# Missing values < 5, reassign
# Missing values >= 5, randomForest/boosting

# Values with < 5 missing values, but the distribution of categories was heavily skewed, we filled it in with the mode
# Electrical - 2 NA's
  # Visual
electrical <- ggplot(data = combined, aes(Electrical)) +  geom_bar(aes(fill = Electrical), show.legend = FALSE)  + scale_fill_brewer(palette="Blues")
combined$Electrical[is.na(combined$Electrical)] = "SBrkr"

# KitchenQual - 2 NA's
  # Visual
kitchenqual <- ggplot(data = combined, aes(KitchenQual)) +  geom_bar(aes(fill = KitchenQual), show.legend = FALSE) + scale_fill_brewer(palette="BrBG")
combined$KitchenQual[is.na(combined$KitchenQual)] = "TA"

# Functional - 3 NA's
  # Visual
functional <- ggplot(data = combined, aes(Functional)) +  geom_bar(aes(fill = Functional), show.legend = FALSE) + scale_fill_brewer(palette="PiYG")
combined$Functional[is.na(combined$Functional)] = "Typ"

# Exterior 1st - 1 NA
  # Visual
exterior1st <- ggplot(data = combined, aes(Exterior1st)) +  geom_bar(aes(fill = Exterior1st), show.legend = FALSE) + scale_fill_brewer(palette="BrBG") + theme(axis.text.x = element_text(angle = 45, hjust = 1))
combined$Exterior1st[is.na(combined$Exterior1st)] = "VinylSd"

# Exterior 2nd - 1 NA
  # Visual
exterior2nd <- ggplot(data = combined, aes(Exterior2nd)) +  geom_bar(aes(fill = Exterior2nd), show.legend = FALSE)  + scale_fill_brewer(palette="YlGn") + theme(axis.text.x = element_text(angle = 45, hjust = 1))
combined$Exterior2nd[is.na(combined$Exterior2nd)] = "VinylSd"
# Sale Type - 1 NA
 # Visual
saletype<- ggplot(data = combined, aes(SaleType)) +  geom_bar(aes(fill = SaleType), show.legend = FALSE)  +  scale_fill_brewer(palette="Spectral") + theme(axis.text.x = element_text(angle = 45, hjust = 1))
# Since SaleType is correlated w/ SaleCondition, we see Normal is most associated with "WD"
combined[is.na(combined$SaleType),c('SaleType', 'SaleCondition')]
table(combined$SaleCondition, combined$SaleType) # table showing Normal & "WD"
ggplot(combined, aes(SaleCondition)) + geom_bar(aes(fill = SaleType)) # graph showing Normal & "WD"
# Replace all NA sale type w/ "WD"
combined$SaleType[is.na(combined$SaleType)] = 'WD'


# Figure 4
figure <- ggarrange(electrical, kitchenqual, functional, exterior1st, exterior2nd, saletype,
          labels = c("A", "B", "C", "D", "E","F"),
          ncol = 2, nrow = 3)
fig4 <- annotate_figure(figure,
                top = text_grob("Exploratory Analysis for Imputation", face = "bold", size = 12, hjust = 0.5),
                bottom = text_grob(" All figures: There is a clear factor level where the majority of the observations lie. We assign the missing \n values for each predictor to the level with the highest frequency.",hjust = 0, x = 0, face = "italic", size = 10),
                fig.lab = "Figure 4", fig.lab.face = "bold"
                )
ggexport(plotlist = list(fig4),filename = "figure4.png")
rm(list = c("electrical", "kitchenqual", "functional", "exterior1st", "exterior2nd","saletype", "figure", "fig4"))

##################################################
# Missing values >= 5
# Prediction Model
# Getting the data without NA's
NoNaData = combined[,colSums(is.na(combined))==0]
NoNaNumeric = select_if(NoNaData, is.numeric) # numeric
NoNaCat = select_if(NoNaData, is.factor) # categorical

# Replacing MSZoning NA's
MSdata = cbind(MSZoning=combined$MSZoning,NoNaData)
MStrain = MSdata[complete.cases(MSdata),]
MStest = MSdata[is.na(MSdata),]
rfMS = randomForest(MSZoning~.,data=MStrain,mtry=sqrt(74),importance=TRUE)
pred.MS = predict(rfMS,newdata=MStest)
combined$MSZoning[is.na(combined$MSZoning)] = pred.MS

# Replacing LotFrontage NA
library(gbm)
Lotdata = cbind(LotFrontage=combined$LotFrontage,NoNaData)
Lottrain = Lotdata[complete.cases(Lotdata),]
Lottest = Lotdata[is.na(Lotdata),]
n = nrow(Lottrain)
traini = sample(1:n,2930,replace=FALSE)
Lottn = Lottrain[traini,]
Lottt = Lottrain[-traini,]
Lotgbm = gbm(LotFrontage~.,data=Lottrain,distribution="gaussian",n.tree=500,interaction.depth=4)
pred.Lot = predict(Lotgbm,newdata=Lottest,n.tree=500)
combined$LotFrontage[is.na(combined$LotFrontage)] = pred.Lot
##################################################

###### Feature Engineering
# House Age And Yes/No for remodeled
# Remodeled (Categorical)
combined$Remodeled <- rep("Yes", nrow(combined))
combined$Remodeled[which (combined$YearRemodAdd - combined$YearBuilt==0)]<- "No"
combined$Remodeled <- as.factor(combined$Remodeled)
# House Age
# age = 2018 - yearbuilt (if not remodeled)
combined$HouseAge[which(combined$Remodeled == "No")] <- 2018 - combined$YearBuilt[which(combined$Remodeled == "No")]
# age = 2018 - yearRemodeled (if remodeled)
combined$HouseAge[which(combined$Remodeled == "Yes")] <- 2018 - combined$YearRemodAdd[which(combined$Remodeled == "Yes")]
# We can drop YearBuilt, YearRemodAdd 
combined = combined[,colnames(combined) != "YearBuilt"]
combined = combined[,colnames(combined) != "YearRemodAdd"]

# Taking out BsmtFinSF1 and 2
combined$BsmtTotalFin = combined$BsmtFinSF1 + combined$BsmtFinSF2
# We can drop BsmtFinSF1, BsmtFinSF2
combined = combined[,colnames(combined) != "BsmtFinSF2"]
combined = combined[,colnames(combined) != "BsmtFinSF1"]

# Condense levels of LotShape
combined$LotShape <- as.character(combined$LotShape)
combined$LotShape[which (combined$LotShape!= "Reg")] <- "IR"
combined$LotShape <- as.factor(combined$LotShape)

# Reorder our columns so that affordabilitty is last predictor
combined2 <- combined
combined <- cbind(combined2[,-73], affordabilitty = combined2$affordabilitty)
rm(combined2)
```

Correlation between numeric predictors
```{r}
# Splitting combined by numeric and factors
combined.numeric <- select_if(combined, is.numeric)
combined.factors =select_if(combined, is.factor) 

# Correlation between our numeric predictors
library(corrplot)
library(reshape2)
# Get upper triangle of the correlation matrix
  get_upper_tri <- function(cormat){
    cormat[lower.tri(cormat)]<- NA
    return(cormat)
  }
corr <- round(cor(combined.numeric),3)
# Visualize correlation matrix
upper_tri <- get_upper_tri(corr)
melted_corr <- melt(upper_tri, na.rm = TRUE)
corr_plot <- ggplot(data = melted_corr, aes(Var2, Var1, fill = value))+
 geom_tile(color = "white")+
 scale_fill_gradient2(low = "blue", high = "red", mid = "white", 
   midpoint = 0, limit = c(-1,1), space = "Lab", 
   name="Pearson\nCorrelation") +
  theme_minimal()+ 
 theme(axis.text.x = element_text(angle = 60, vjust = 1, 
    size = 8, hjust = 1))+ coord_fixed() +  theme(axis.title.x = element_blank()) + theme(axis.title.y = element_blank())
corr_plot

corr_plot_annotate<- annotate_figure(corr_plot,
                top = text_grob("Correlation Matrix of Numeric Predictors", face = "bold", size = 12, hjust = 0.5),
                bottom = text_grob("Correlation matrix of the numeric predictors for both the training and testing set.",hjust = 0, x = 0, face = "italic", size = 10),
                fig.lab = "Figure 5", fig.lab.face = "bold"
                )
ggexport(plotlist = list(corr_plot_annotate),filename = "corrplot.png")

# See which predictors have a correlation equal to or higher than 0.7
corr[which(abs(corr) >=0.7 & abs(corr) !=1 )]
which(abs(corr) >=0.7 & abs(corr) !=1 )
temp_ind <- which(abs(corr) >=0.7 & abs(corr) !=1  , arr.ind = TRUE)
corr_table <- data.frame(predictors = paste(rownames(corr)[temp_ind[,"row"]],  colnames(corr)[temp_ind[,"col"]], sep=', '), values = corr[which(abs(corr) >=0.7 & abs(corr) !=1 )])
corr_table

rm(list = c("corr", "upper_tri", "melted_corr", "corr_plot", "corr_plot_annotate", "temp_ind"))

# Highly correlated pairs:
# We drop GarageCars, GarageArea
# We drop X1stFlrSF, TotalBsmtSF
# We drop GrLivArea, TotRmsAbvGrd

training = combined[1:3500,]
testing = combined[3501:5000,]
training = training[complete.cases(training),]

# Visualizing different densities w/ training data
# GarageCars, GarageArea scatter plot colored by affordabilitty
garage <- ggscatter(training, x = "GarageArea", y = "GarageCars",
                    color = "affordabilitty", palette = "jco",
                    size = 3, alpha = 0.6) + border()                                         
# Marginal density plot of x (top panel) and y (right panel)
x1plot <- ggdensity(training, "GarageArea", fill = "affordabilitty",
                    palette = "jco")
y1plot <- ggdensity(training, "GarageCars", fill = "affordabilitty", 
                    palette = "jco")+ rotate()
# Cleaning the plots
y1plot <- y1plot + clean_theme() 
x1plot <- x1plot + clean_theme()

# X1stFlrSF, TotalBsmtSF scatter plot colored by affordabilitty
sf <- ggscatter(training, x = "X1stFlrSF", y = "TotalBsmtSF",
                color = "affordabilitty", palette = "jco",
                size = 3, alpha = 0.6) + border()                                         
x2plot <- ggdensity(training, "X1stFlrSF", fill = "affordabilitty",
                    palette = "jco") 
y2plot <- ggdensity(training, "TotalBsmtSF", fill = "affordabilitty", 
                    palette = "jco")+ rotate()
# Cleaning the plots
y2plot <- y2plot + clean_theme() 
x2plot <- x2plot + clean_theme()

# GrLivArea, TotRmsAbvGrd scatter plot colored by affordabilitty
gr <- ggscatter(training, x = "GrLivArea", y = "TotRmsAbvGrd",
                color = "affordabilitty", palette = "jco",
                size = 3, alpha = 0.6) + border()                                         
x3plot <- ggdensity(training, "GrLivArea", fill = "affordabilitty",
                    palette = "jco") 
y3plot <- ggdensity(training, "TotRmsAbvGrd", fill = "affordabilitty", 
                    palette = "jco")+ rotate()
# Cleaning the plots
y3plot <- y3plot + clean_theme() 
x3plot <- x3plot + clean_theme()

# Arranging the plot
combined <- ggarrange(x1plot, NULL, x2plot, NULL, garage, y1plot, sf, y2plot,  x3plot, NULL, NULL, NULL, gr, y3plot,
                      ncol = 4, nrow = 4,  align = "hv", 
                      widths = c(2, 1), heights = c(1, 2),
                      common.legend = TRUE, legend = "bottom",
                      labels = c("A","","B","","" ,"","","", "C")
)

combined_annotate<- annotate_figure(combined,
                top = text_grob("Density Plots of Highly Correlated Variables", face = "bold", size = 17, hjust = 0.5),
                fig.lab = "Figure 6", fig.lab.face = "bold", fig.lab.size = 13
                )
ggexport(plotlist = list(combined_annotate), height = 900, width = 900, filename = "density_highly_correlated.jpeg")

rm(list = c("garage", "x1plot", "y1plot", "sf", "x2plot", "y2plot", "gr","x3plot", "y3plot","combined", "combined_annotate"))

# Drop GarageCars, X1stFlrSF, and TotRmsAbvGrd from our combined df
combined.numeric = combined.numeric[,colnames(combined.numeric) != "GarageCars"]
combined.numeric = combined.numeric[,colnames(combined.numeric) != "X1stFlrSF"]
combined.numeric = combined.numeric[,colnames(combined.numeric) != "TotRmsAbvGrd"]
```


PCA on the numeric variables
```{r}
library(factoextra)
library(data.table)
library(formattable)

pca = princomp(combined.numeric,cor=T)
eig.val <- get_eigenvalue(pca)
eig.val
scree.plot <- fviz_eig(pca, addlabels = TRUE, ylim = c(0,30), main = "" ) # to control how many dimensions to show ncp = 25
scree.plot
# Print scree plot
scree.plot_annotated<- annotate_figure(scree.plot,
                top = text_grob("PCA Scree Plot", face = "bold", size = 12, hjust = 0.5),
                fig.lab = "Figure 7", fig.lab.face = "bold"
                )
ggexport(plotlist = list(scree.plot_annotated), width = 1000,filename = "scree.plot.jpeg")



# Make our eig value table pretty
customBlack = "#000"
customGreen = "#71CA97"
customRed = "#ff7f7f"
names(eig.val) <- c("Eigenvalue", "Variance %", "Cumulative Variance %")
eig.val <- round(eig.val, 3)
rownames(eig.val) <- c(1:29)
eig.val$Dimension <- seq(1:29)
eig.val <- eig.val[,c(4,1,2,3)]

eigenvalue_formatter <- formatter("span", style = x ~ style( 
              color = ifelse(x > 1, customGreen, ifelse(x < 1, "black", "black"))))

variance_formatter <- formatter("span", style = x ~ style( 
              color = ifelse(x > 95, customGreen, ifelse(x < 95, customRed, "black"))))

eig.table <- formattable(eig.val, align = c("l", "c","c","r"),
            list(`Dimension` = formatter("span", style = ~ style(color = "grey",font.weight = "bold")), 
                 `Eigenvalue` = eigenvalue_formatter,
                 `Cumulative Variance %` = variance_formatter))
eig.table
ggexport(plotlist = list(eig.table),filename = "eigtable.png")

x = summary(pca)
pc.comp <- pca$scores
pcdf = pc.comp[,1:23]
pcdf = data.frame(pcdf)
colnames(pcdf) = LETTERS[1:23]

# Our final dataframe after PCA
combined_pca = cbind(combined.factors[,-44],pcdf, affordabilitty = combined.factors$affordabilitty)

# Why did we drop exterior2nd
# combined_pca = training.factors[,colnames(training.factors) != "Exterior2nd"]

# Split our PCA combined data into the original testing and training set
testing1 = combined_pca[3501:5000,]
training1 = combined_pca[1:3500,]
training1 = training1[complete.cases(training1),]
```


Splitting the training to 90% and 10%
```{r}
set.seed(1234)
traini = sample(nrow(training1),nrow(training1)*0.9,replace=FALSE)
train_train = training1[traini,]
train_test = training1[-traini,]
```

############

Logistic Model With all predictors included using Cleaned Data
```{r}
m2 = glm(affordabilitty~.,family=binomial(),data=train_train)


predictprobs = predict(m2,train_test, type='response')
predictlogit = rep('Affordable',length(predictprobs))
predictlogit[predictprobs>0.5] = 'Unaffordable'
table(predictlogit,train_test$affordabilitty)
mean(predictlogit==train_test$affordabilitty)
```



Tree and Randomforest
```{r}
require(tree)
library(randomForest)  
library(e1071)  
library(caret)  
library(ggplot2)  
library(MASS)
library(ISLR)
library(rfUtilities)


full=tree(affordabilitty~.,mindev=0,minsize=2,data=train_train)
pred.full=predict(full,newdata=train_test,type="class")
table(pred.full,train_test$affordabilitty)
mean(pred.full==train_test$affordabilitty)

# Seeing which tree is best
cvtree = cv.tree(full,FUN=prune.misclass)
df = data.frame(size=cvtree$size,dev = cvtree$dev)
df=df[order(df$dev),]
df
pruned = prune.misclass(full,best=2)

#Finding Results of Pruned Tree
pred.pr = predict(pruned,newdata=train_test,type="class")
table(pred.pr,train_test$affordabilitty)
mean(pred.pr == train_test$affordabilitty)


# Bagging
rm1 = randomForest(affordabilitty~.,data=train_train,mtry=66,importance=TRUE,ntree=2000)
pred.rm=predict(rm1,data=train_train)
table(train_train$affordabilitty,pred.rm)
mean(train_train$affordabilitty==pred.rm)

pred.rm=predict(rm1,newdata=train_test)
table(train_test$affordabilitty,pred.rm)
mean(train_test$affordabilitty==pred.rm)

df = importance(rm1)
df = as.data.frame(df)
df$rname = rownames(df)
df = arrange(df,desc(MeanDecreaseAccuracy))
df = df[,c(5,1,2,3,4)]
df1 = arrange(df,desc(MeanDecreaseGini))

# df of Mean Decrease Accuracy Best
MostImportantVar = c(df[1:5,1],df1[1:5,1])
x = which((table(MostImportantVar)==2)==TRUE)
x1 = names(x)
x1

m2 = randomForest(affordabilitty~A+BsmtFinType1+C+E+L+M+Neighborhood+S+W,data=train_train,mtry=sqrt(9),importance=TRUE,ntree=1300)
plot(m2)

m3 = randomForest(affordabilitty~A+BsmtFinType1+C+D+E+J+L+M+Neighborhood+O+S+W,data=train_train,mtry=sqrt(12),importance=TRUE,ntree=1200)
plot(m3)

m4 = randomForest(affordabilitty~A+C+E+Neighborhood+S+W,data=train_train,mtry=sqrt(6),importance=TRUE,ntree=1200)
plot(m4)

m5 = randomForest(affordabilitty~A+C+Neighborhood+W,data=train_train,mtry=sqrt(4),importance=TRUE,ntree=1000)
plot(m5)

#Predicting Using M2
pred.rm = predict(m2,data=train_train)
table(train_train$affordabilitty,pred.rm)
mean(train_train$affordabilitty==pred.rm)

pred.rm = predict(m2,newdata=train_test)
table(train_test$affordabilitty,pred.rm)
mean(train_test$affordabilitty==pred.rm)

#Predicting Using M3
pred.rm = predict(m3,data=train_train)
table(train_train$affordabilitty,pred.rm)
mean(train_train$affordabilitty==pred.rm)

pred.rm = predict(m3,newdata=train_test)
table(train_test$affordabilitty,pred.rm)
mean(train_test$affordabilitty==pred.rm)

#Predicting Using M4 (Best among Models)
pred.rm = predict(m4,data=train_train)
table(train_train$affordabilitty,pred.rm)
mean(train_train$affordabilitty==pred.rm)

pred.rm = predict(m4,newdata=train_test)
table(train_test$affordabilitty,pred.rm)
mean(train_test$affordabilitty==pred.rm)

#Predicting Using M5
pred.rm = predict(m5,data=train_train)
table(train_train$affordabilitty,pred.rm)
mean(train_train$affordabilitty==pred.rm)

pred.rm = predict(m5,newdata=train_test)
table(train_test$affordabilitty,pred.rm)
mean(train_test$affordabilitty==pred.rm)

dfMultiple = data.frame(A=rep(NA,nrow(train_test)), B = rep(NA,nrow(train_test)), C = rep(NA,nrow(train_test)), D = rep(NA,nrow(train_test)),E = rep(NA,nrow(train_test)), F= rep(NA,nrow(train_test)))


for(i in 1:6){
  m4 = randomForest(affordabilitty~A+C+E+Neighborhood+S+W,data=train_train,mtry=sqrt(6),importance=TRUE,ntree=1200)
  dfMultiple[,i] = predict(m2,newdata=train_test)
  dfMultiple[,i] = as.factor(dfMultiple[,i])
  dfMultiple[,i] = revalue(dfMultiple[,i],c("Unaffordable"="0","Affordable"="1"))
  dfMultiple[,i] = as.numeric(as.character(dfMultiple[,i]))
}


for(i in 1:nrow(dfMultiple)){
  dfMultiple$Best[i] = names(sort(table(as.numeric(dfMultiple[i,1:6])),decreasing=TRUE))[1]
}

dfMultiple$Best = as.factor(dfMultiple$Best)
dfMultiple$Best = revalue(dfMultiple$Best,c("0"="Unaffordable","1"="Affordable")) # Results for prediction of testing using training data

table(train_test$affordabilitty,dfMultiple$Best)
mean(train_test$affordabilitty==dfMultiple$Best)
```


TESTING
```{r}
m4 = randomForest(affordabilitty~A+C+E+Neighborhood+S+W,data=training1,mtry=sqrt(6),importance=TRUE,ntree=1200)
plot(m4)

dfMultiple = data.frame(A=rep(NA,nrow(testing1)), B = rep(NA,nrow(testing1)), C = rep(NA,nrow(testing1)), D = rep(NA,nrow(testing1)),E = rep(NA,nrow(testing1)), F= rep(NA,nrow(testing1)))


for(i in 1:6){
  m4 = randomForest(affordabilitty~A+C+E+Neighborhood+S+W,data=training1,mtry=sqrt(6),importance=TRUE,ntree=1200)
  dfMultiple[,i] = predict(m2,newdata=testing1)
  dfMultiple[,i] = as.factor(dfMultiple[,i])
  dfMultiple[,i] = revalue(dfMultiple[,i],c("Unaffordable"="0","Affordable"="1"))
  dfMultiple[,i] = as.numeric(as.character(dfMultiple[,i]))
}


for(i in 1:nrow(dfMultiple)){
  dfMultiple$Best[i] = names(sort(table(as.numeric(dfMultiple[i,1:6])),decreasing=TRUE))[1]
}



dfMultiple$Best = as.factor(dfMultiple$Best)
dfMultiple$Best = revalue(dfMultiple$Best,c("0"="Unaffordable","1"="Affordable")) #Results for prediction of actual testing data

```



